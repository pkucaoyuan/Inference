#!/usr/bin/env python3
"""
å®é™…æ¨ç†åŸºå‡†æµ‹è¯•è„šæœ¬
æµ‹é‡FLUXå’ŒLuminaçš„å®é™…GPUæ¨ç†æ—¶é—´
"""

import os
import time
import torch
import json
import numpy as np
from typing import Dict, List, Tuple
import psutil
try:
    import GPUtil
    GPUTIL_AVAILABLE = True
except ImportError:
    GPUTIL_AVAILABLE = False
    print("è­¦å‘Š: GPUtilæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆè·å–GPUä¿¡æ¯")
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥å¿…è¦çš„åº“
try:
    from diffusers import FluxPipeline, StableDiffusionXLPipeline
    from transformers import AutoTokenizer, AutoModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False
    print("è­¦å‘Š: diffusersåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

class InferenceBenchmark:
    """æ¨ç†åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self, output_dir: str = "./output_images"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.results = []
        
        # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€çš„è¾“å‡ºç›®å½•
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"{output_dir}_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # æµ‹è¯•é…ç½®
        self.test_prompts = [
            "A beautiful landscape with mountains and lakes, photorealistic",
            "A futuristic city with flying cars, cyberpunk style",
            "A cute anime character in a magical garden, detailed"
        ]
        
        self.test_sizes = [
            (1024, 1024)  # åªæµ‹è¯•1024å°ºå¯¸
        ]
        
        # æ ¹æ®å®˜æ–¹æ¨èè®¾ç½®æµ‹è¯•æ­¥æ•°
        self.model_recommended_steps = {
            "FLUX": [50],               # FLUXå®˜æ–¹ç¤ºä¾‹ä½¿ç”¨50æ­¥
            "Lumina": [30]              # Luminaé»˜è®¤30æ­¥
        }
    
    def benchmark_flux(self) -> Dict:
        """åŸºå‡†æµ‹è¯•FLUXæ¨¡å‹"""
        print("å¼€å§‹æµ‹è¯•FLUXæ¨¡å‹...")
        
        if not DIFFUSERS_AVAILABLE:
            print("é”™è¯¯: diffusersåº“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒçœŸå®æ¨ç†æµ‹è¯•")
            return None
        
        # ç›´æ¥è°ƒç”¨çœŸå®æµ‹è¯•å‡½æ•°
        return self._real_flux_benchmark()
    
    def benchmark_lumina(self) -> Dict:
        """åŸºå‡†æµ‹è¯•Luminaæ¨¡å‹"""
        print("å¼€å§‹æµ‹è¯•Luminaæ¨¡å‹...")
        
        if not DIFFUSERS_AVAILABLE:
            print("é”™è¯¯: diffusersåº“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒçœŸå®æ¨ç†æµ‹è¯•")
            return None
        
        # ç›´æ¥è°ƒç”¨çœŸå®æµ‹è¯•å‡½æ•°
        return self._real_lumina_benchmark()
    
    
    def _benchmark_single_inference(self, pipe, prompt: str, size: Tuple[int, int], 
                                  steps: int, model_name: str) -> Dict:
        """å•æ¬¡æ¨ç†åŸºå‡†æµ‹è¯•"""
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        start_memory = self._get_gpu_memory_nvidia_smi()
        if start_memory == 0.0:
            # å¦‚æœnvidia-smiå¤±è´¥ï¼Œä½¿ç”¨PyTorchçš„CUDAå†…å­˜ç›‘æ§
            start_memory = self._get_gpu_memory()
            print(f"ğŸ” ä½¿ç”¨PyTorch CUDAå†…å­˜ç›‘æ§: {start_memory:.2f}GB")
        
        try:
            # è®°å½•æ¨ç†å¼€å§‹æ—¶é—´
            inference_start_time = time.time()
            
            # æ‰§è¡Œæ¨ç† - ä½¿ç”¨å®˜æ–¹æ¨èå‚æ•°
            if model_name == "FLUX":
                # FLUXå®˜æ–¹ç¤ºä¾‹å‚æ•°
                image = pipe(
                    prompt,
                    height=size[0],
                    width=size[1],
                    guidance_scale=3.5,
                    num_inference_steps=steps,
                    max_sequence_length=512,
                    generator=torch.Generator("cpu").manual_seed(0)
                ).images[0]
            elif model_name == "Lumina":
                # Luminaå®˜æ–¹é»˜è®¤å‚æ•°
                image = pipe(
                    prompt,
                    height=size[0],
                    width=size[1],
                    num_inference_steps=steps,
                    guidance_scale=4.0,
                    cfg_trunc_ratio=1.0,  # å®˜æ–¹é»˜è®¤å€¼
                    cfg_normalization=True,
                    max_sequence_length=256
                ).images[0]
            
            # è®°å½•æ¨ç†ç»“æŸæ—¶é—´
            inference_end_time = time.time()
            inference_time = inference_end_time - inference_start_time
            
            # ä¿å­˜ç”Ÿæˆçš„å›¾ç‰‡
            save_start_time = time.time()
            safe_prompt = "".join(c for c in prompt[:30] if c.isalnum() or c in (' ', '-', '_')).rstrip()
            filename = f"{model_name.lower().replace(' ', '_')}_{size[0]}x{size[1]}_steps_{steps}_cfg_{3.5 if model_name == 'FLUX' else 4.0 if model_name == 'Lumina' else 4.5}_{safe_prompt}.png"
            image_path = self.output_dir / filename
            image.save(image_path)
            save_time = time.time() - save_start_time
            print(f"ä¿å­˜å›¾ç‰‡: {image_path} (è€—æ—¶: {save_time:.2f}ç§’)")
            
            # è®°å½•ç»“æŸçŠ¶æ€
            end_time = time.time()
            end_memory = self._get_gpu_memory_nvidia_smi()
            if end_memory == 0.0:
                # å¦‚æœnvidia-smiå¤±è´¥ï¼Œä½¿ç”¨PyTorchçš„CUDAå†…å­˜ç›‘æ§
                end_memory = self._get_gpu_memory()
                print(f"ğŸ” ä½¿ç”¨PyTorch CUDAå†…å­˜ç›‘æ§: {end_memory:.2f}GB")
            
            return {
                'prompt': prompt,
                'size': size,
                'steps': steps,
                'inference_time': inference_time,  # ä½¿ç”¨çº¯æ¨ç†æ—¶é—´
                'total_time': end_time - start_time,  # æ€»æ—¶é—´ï¼ˆåŒ…æ‹¬ä¿å­˜ï¼‰
                'save_time': save_time,  # ä¿å­˜æ—¶é—´
                'gpu_memory': end_memory,  # ä½¿ç”¨å®é™…ä½¿ç”¨çš„å†…å­˜ï¼Œè€Œä¸æ˜¯å˜åŒ–é‡
                'success': True
            }
            
        except Exception as e:
            end_time = time.time()
            end_memory = self._get_gpu_memory_nvidia_smi()
            if end_memory == 0.0:
                end_memory = self._get_gpu_memory()
            return {
                'prompt': prompt,
                'size': size,
                'steps': steps,
                'inference_time': end_time - start_time,
                'total_time': end_time - start_time,
                'save_time': 0.0,
                'gpu_memory': end_memory,  # è®°å½•å®é™…ä½¿ç”¨çš„å†…å­˜
                'success': False,
                'error': str(e)
            }
    
    def _real_flux_benchmark(self) -> Dict:
        """çœŸå®FLUXåŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹çœŸå®FLUXæ¨¡å‹æµ‹è¯•...")
        
        try:
            # å°è¯•åŠ è½½FLUXæ¨¡å‹
            from diffusers import FluxPipeline
            
            print("æ­£åœ¨åŠ è½½FLUXæ¨¡å‹...")
            pipe = FluxPipeline.from_pretrained(
                "./FLUX.1-dev",
                torch_dtype=torch.bfloat16,
                device_map="cuda"  # ä½¿ç”¨cudaè€Œä¸æ˜¯auto
            )
            
            results = []
            for prompt in self.test_prompts:
                for size in self.test_sizes:
                    for steps in self.model_recommended_steps["FLUX"]:
                        result = self._benchmark_single_inference(
                            pipe, prompt, size, steps, "FLUX"
                        )
                        results.append(result)
            
            return {
                'model': 'FLUX (Real Test)',
                'results': results,
                'avg_time': np.mean([r['inference_time'] for r in results]),
                'avg_memory': np.mean([r['gpu_memory'] for r in results])
            }
            
        except Exception as e:
            print(f"FLUXçœŸå®æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    def _real_lumina_benchmark(self) -> Dict:
        """çœŸå®LuminaåŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹çœŸå®Luminaæ¨¡å‹æµ‹è¯•...")
        
        try:
            # å°è¯•åŠ è½½Luminaæ¨¡å‹
            from diffusers import Lumina2Pipeline
            
            print("æ­£åœ¨åŠ è½½Luminaæ¨¡å‹...")
            pipe = Lumina2Pipeline.from_pretrained(
                "./Lumina-Image-2.0",
                torch_dtype=torch.bfloat16
            )
            pipe.enable_model_cpu_offload()
            
            results = []
            for prompt in self.test_prompts:
                for size in self.test_sizes:
                    for steps in self.model_recommended_steps["Lumina"]:
                        result = self._benchmark_single_inference(
                            pipe, prompt, size, steps, "Lumina"
                        )
                        results.append(result)
            
            return {
                'model': 'Lumina (Real Test)',
                'results': results,
                'avg_time': np.mean([r['inference_time'] for r in results]),
                'avg_memory': np.mean([r['gpu_memory'] for r in results])
            }
            
        except Exception as e:
            print(f"LuminaçœŸå®æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    
    
    
    def _get_gpu_memory(self) -> float:
        """è·å–GPUå†…å­˜ä½¿ç”¨é‡"""
        if torch.cuda.is_available():
            # è·å–å½“å‰åˆ†é…çš„å†…å­˜
            allocated = torch.cuda.memory_allocated() / (1024**3)  # GB
            # è·å–ç¼“å­˜çš„å†…å­˜
            cached = torch.cuda.memory_reserved() / (1024**3)  # GB
            return allocated + cached
        return 0.0
    
    def _get_gpu_memory_nvidia_smi(self) -> float:
        """ä½¿ç”¨nvidia-smiè·å–GPUå†…å­˜ä½¿ç”¨é‡"""
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                # å¤„ç†å¤šè¡Œè¾“å‡ºï¼Œå–ç¬¬ä¸€è¡Œ
                lines = result.stdout.strip().split('\n')
                if lines and lines[0]:
                    memory_mb = float(lines[0].strip())
                    memory_gb = memory_mb / 1024.0  # è½¬æ¢ä¸ºGB
                    print(f"ğŸ” GPUå†…å­˜ç›‘æ§: {memory_mb:.0f}MB ({memory_gb:.2f}GB)")
                    return memory_gb
                else:
                    print("âš ï¸ nvidia-smiè¾“å‡ºä¸ºç©º")
            else:
                print(f"âš ï¸ nvidia-smiå‘½ä»¤å¤±è´¥: {result.stderr}")
        except Exception as e:
            print(f"âš ï¸ GPUå†…å­˜ç›‘æ§å¼‚å¸¸: {e}")
        return 0.0
    
    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•...")
        
        # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
        flux_results = self.benchmark_flux()
        lumina_results = self.benchmark_lumina()
        
        # åªæ”¶é›†æˆåŠŸçš„ç»“æœ
        self.results = []
        if flux_results:
            self.results.append(flux_results)
        if lumina_results:
            self.results.append(lumina_results)
        
        if not self.results:
            print("é”™è¯¯: æ‰€æœ‰æ¨¡å‹æµ‹è¯•éƒ½å¤±è´¥äº†ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return []
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_benchmark_report()
        
        return self.results
    
    def generate_benchmark_report(self):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        print("ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š...")
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = Path("benchmark_report")
        report_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        self._generate_text_report(report_dir)
        
        # ç”Ÿæˆå›¾è¡¨
        self._generate_benchmark_charts(report_dir)
        
        # ç”ŸæˆJSONæ•°æ®
        self._generate_json_data(report_dir)
        
        print(f"åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆåˆ°: {report_dir}")
    
    def _generate_text_report(self, report_dir: Path):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        report_path = report_dir / "benchmark_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("æ¨¡å‹æ¨ç†åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            for result in self.results:
                f.write(f"æ¨¡å‹: {result['model']}\n")
                f.write(f"å¹³å‡æ¨ç†æ—¶é—´: {result['avg_time']:.2f}ç§’\n")
                f.write(f"å¹³å‡GPUå†…å­˜ä½¿ç”¨: {result['avg_memory']:.2f}GB\n")
                f.write("-" * 30 + "\n")
                
                # è¯¦ç»†ç»“æœ
                for r in result['results']:
                    f.write(f"  æç¤ºè¯: {r['prompt'][:50]}...\n")
                    f.write(f"  å°ºå¯¸: {r['size']}\n")
                    f.write(f"  æ­¥æ•°: {r['steps']}\n")
                    f.write(f"  æ¨ç†æ—¶é—´: {r['inference_time']:.2f}ç§’\n")
                    f.write(f"  GPUå†…å­˜: {r['gpu_memory']:.2f}GB\n")
                    f.write(f"  æˆåŠŸ: {r['success']}\n\n")
    
    def _generate_benchmark_charts(self, report_dir: Path):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•å›¾è¡¨"""
        if not self.results:
            return
        
        # è®¾ç½®å­—ä½“ï¼ˆå…¼å®¹ä¸åŒç³»ç»Ÿï¼‰
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
        except:
            # å¦‚æœä¸­æ–‡å­—ä½“ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Average Inference Time Comparison
        models = [r['model'] for r in self.results]
        avg_times = [r['avg_time'] for r in self.results]
        
        axes[0, 0].bar(models, avg_times, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[0, 0].set_title('Average Inference Time Comparison')
        axes[0, 0].set_ylabel('Time (seconds)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. Average GPU Memory Usage Comparison
        avg_memory = [r['avg_memory'] for r in self.results]
        
        axes[0, 1].bar(models, avg_memory, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[0, 1].set_title('Average GPU Memory Usage Comparison')
        axes[0, 1].set_ylabel('Memory (GB)')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # 3. Inference Time Distribution
        all_times = []
        all_models = []
        for result in self.results:
            for r in result['results']:
                all_times.append(r['inference_time'])
                all_models.append(result['model'])
        
        # Create box plot
        model_times = {}
        for model, time in zip(all_models, all_times):
            if model not in model_times:
                model_times[model] = []
            model_times[model].append(time)
        
        axes[1, 0].boxplot([model_times[model] for model in models], labels=models)
        axes[1, 0].set_title('Inference Time Distribution')
        axes[1, 0].set_ylabel('Time (seconds)')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. Efficiency Comparison (Time/Memory)
        efficiency = [t/m if m > 0 else 0 for t, m in zip(avg_times, avg_memory)]
        
        axes[1, 1].bar(models, efficiency, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[1, 1].set_title('Inference Efficiency Comparison (Time/Memory)')
        axes[1, 1].set_ylabel('Efficiency Metric')
        axes[1, 1].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig(report_dir / "benchmark_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_json_data(self, report_dir: Path):
        """ç”ŸæˆJSONæ•°æ®"""
        json_path = report_dir / "benchmark_data.json"
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

def main():
    """ä¸»å‡½æ•°"""
    print("æ¨¡å‹æ¨ç†åŸºå‡†æµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
    benchmark = InferenceBenchmark()
    
    # è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
    results = benchmark.run_all_benchmarks()
    
    print("\nåŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("è¯·æŸ¥çœ‹ benchmark_report ç›®å½•ä¸­çš„è¯¦ç»†æŠ¥å‘Šã€‚")

if __name__ == "__main__":
    main()
