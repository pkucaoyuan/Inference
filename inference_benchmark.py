#!/usr/bin/env python3
"""
å®é™…æ¨ç†åŸºå‡†æµ‹è¯•è„šæœ¬
æµ‹é‡FLUXå’ŒLuminaçš„å®é™…GPUæ¨ç†æ—¶é—´
"""

import os
import time
import torch
import json
import numpy as np
from typing import Dict, List, Tuple
import psutil
try:
    import GPUtil
    GPUTIL_AVAILABLE = True
except ImportError:
    GPUTIL_AVAILABLE = False
    print("è­¦å‘Š: GPUtilæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆè·å–GPUä¿¡æ¯")
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥å¿…è¦çš„åº“
try:
    from diffusers import FluxPipeline, StableDiffusionXLPipeline
    from transformers import AutoTokenizer, AutoModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False
    print("è­¦å‘Š: diffusersåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

class InferenceBenchmark:
    """æ¨ç†åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self, output_dir: str = "./output_images"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.results = []
        
        # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€çš„è¾“å‡ºç›®å½•
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"{output_dir}_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # æµ‹è¯•é…ç½®
        self.test_prompts = [
            "A beautiful landscape with mountains and lakes, photorealistic",
            "A futuristic city with flying cars, cyberpunk style",
            "A cute anime character in a magical garden, detailed"
        ]
        
        self.test_sizes = [
            (1024, 1024)  # åªæµ‹è¯•1024å°ºå¯¸
        ]
        
        # æ ¹æ®å®˜æ–¹æ¨èè®¾ç½®æµ‹è¯•æ­¥æ•°
        self.model_recommended_steps = {
            "FLUX": [50],               # FLUXå®˜æ–¹ç¤ºä¾‹ä½¿ç”¨50æ­¥
            "Lumina": [30]              # Luminaé»˜è®¤30æ­¥
        }
    
    def benchmark_flux(self) -> Dict:
        """åŸºå‡†æµ‹è¯•FLUXæ¨¡å‹"""
        print("å¼€å§‹æµ‹è¯•FLUXæ¨¡å‹...")
        
        if not DIFFUSERS_AVAILABLE:
            print("é”™è¯¯: diffusersåº“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒçœŸå®æ¨ç†æµ‹è¯•")
            return None
        
        # ç›´æ¥è°ƒç”¨çœŸå®æµ‹è¯•å‡½æ•°
        return self._real_flux_benchmark()
    
    def benchmark_lumina(self) -> Dict:
        """åŸºå‡†æµ‹è¯•Luminaæ¨¡å‹"""
        print("å¼€å§‹æµ‹è¯•Luminaæ¨¡å‹...")
        
        if not DIFFUSERS_AVAILABLE:
            print("é”™è¯¯: diffusersåº“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒçœŸå®æ¨ç†æµ‹è¯•")
            return None
        
        # ç›´æ¥è°ƒç”¨çœŸå®æµ‹è¯•å‡½æ•°
        return self._real_lumina_benchmark()
    
    
    def _benchmark_single_inference(self, pipe, prompt: str, size: Tuple[int, int], 
                                  steps: int, model_name: str) -> Dict:
        """å•æ¬¡æ¨ç†åŸºå‡†æµ‹è¯•"""
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        
        try:
            # ä½¿ç”¨å®é™…æµ‹é‡æ–¹æ³•
            print(f"å¼€å§‹{model_name}æ¨ç†ï¼ˆå®é™…æµ‹é‡æ¨¡å¼ï¼‰...")
            layer_times = self._measure_actual_layer_times(pipe, prompt, size, steps, model_name)
            
            if layer_times is None:
                raise Exception("å®é™…æµ‹é‡å¤±è´¥")
            
            # è®¡ç®—æ€»æ¨ç†æ—¶é—´
            total_inference_time = sum([
                layer_times.get('text_encoding_time', 0),
                layer_times.get('unet_time', 0),
                layer_times.get('vae_decode_time', 0)
            ])
            
            print(f"æ¨ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_inference_time:.2f}ç§’")
            print(f"  - Text Encoding: {layer_times.get('text_encoding_time', 0):.2f}ç§’")
            print(f"  - UNetæ¨ç†: {layer_times.get('unet_time', 0):.2f}ç§’")
            print(f"    - Attentionå±‚: {layer_times.get('attention_time', 0):.2f}ç§’")
            print(f"    - å…¶ä»–å±‚: {layer_times.get('other_layers_time', 0):.2f}ç§’")
            print(f"  - VAEè§£ç : {layer_times.get('vae_decode_time', 0):.2f}ç§’")
            
            # ä¿å­˜ç”Ÿæˆçš„å›¾ç‰‡
            save_start_time = time.time()
            safe_prompt = "".join(c for c in prompt[:30] if c.isalnum() or c in (' ', '-', '_')).rstrip()
            filename = f"{model_name.lower().replace(' ', '_')}_{size[0]}x{size[1]}_steps_{steps}_cfg_{3.5 if model_name == 'FLUX' else 4.0 if model_name == 'Lumina' else 4.5}_{safe_prompt}.png"
            image_path = self.output_dir / filename
            layer_times['image'].save(image_path)
            save_time = time.time() - save_start_time
            print(f"ä¿å­˜å›¾ç‰‡: {image_path} (è€—æ—¶: {save_time:.2f}ç§’)")
            
            # è®°å½•ç»“æŸçŠ¶æ€
            end_time = time.time()
            
            return {
                'prompt': prompt,
                'size': size,
                'steps': steps,
                'inference_time': total_inference_time,  # ä½¿ç”¨å®é™…æµ‹é‡çš„æ¨ç†æ—¶é—´
                'total_time': end_time - start_time,  # æ€»æ—¶é—´ï¼ˆåŒ…æ‹¬ä¿å­˜ï¼‰
                'save_time': save_time,  # ä¿å­˜æ—¶é—´
                'layer_times': layer_times,  # å®é™…æµ‹é‡çš„å„å±‚æ—¶é—´ç»Ÿè®¡
                'success': True
            }
            
        except Exception as e:
            end_time = time.time()
            print(f"æ¨ç†å¤±è´¥: {e}")
            return {
                'prompt': prompt,
                'size': size,
                'steps': steps,
                'inference_time': end_time - start_time,
                'total_time': end_time - start_time,
                'save_time': 0.0,
                'layer_times': {},
                'success': False,
                'error': str(e)
            }
    
    def _measure_actual_layer_times(self, pipe, prompt: str, size: Tuple[int, int], steps: int, model_name: str) -> Dict:
        """å®é™…æµ‹é‡å„å±‚æ¨ç†æ—¶é—´"""
        try:
            print("å¼€å§‹å®é™…æµ‹é‡å„å±‚æ¨ç†æ—¶é—´...")
            
            # ç›´æ¥ä½¿ç”¨åŸºç¡€æµ‹é‡æ–¹æ³•ï¼Œé¿å…Profilerçš„å¤æ‚æ€§
            print("ä½¿ç”¨åŸºç¡€æµ‹é‡æ–¹æ³•...")
            layer_times = self._fallback_layer_measurement(pipe, prompt, size, steps, model_name)
            
            print(f"å®é™…æµ‹é‡å®Œæˆ:")
            print(f"  - Text Encoding: {layer_times.get('text_encoding_time', 0):.2f}ç§’")
            print(f"  - UNet: {layer_times.get('unet_time', 0):.2f}ç§’")
            print(f"  - VAE Decode: {layer_times.get('vae_decode_time', 0):.2f}ç§’")
            print(f"  - Attention: {layer_times.get('attention_time', 0):.2f}ç§’")
            print(f"  - å…¶ä»–å±‚: {layer_times.get('other_layers_time', 0):.2f}ç§’")
            
            return layer_times
            
        except Exception as e:
            print(f"å®é™…æµ‹é‡å¤±è´¥: {e}")
            # å¦‚æœåŸºç¡€æµ‹é‡å¤±è´¥ï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•
            return self._fallback_layer_measurement(pipe, prompt, size, steps, model_name)
    
    def _analyze_profiler_results(self, prof, model_name: str, steps: int) -> Dict:
        """åˆ†æProfilerç»“æœè·å–å„å±‚æ—¶é—´"""
        try:
            print("è·å–Profileräº‹ä»¶åˆ—è¡¨...")
            # è·å–äº‹ä»¶åˆ—è¡¨
            events = prof.events()
            print(f"Profileräº‹ä»¶æ•°é‡: {len(events)}")
            
            # å¦‚æœäº‹ä»¶è¿‡å¤šï¼Œé™åˆ¶å¤„ç†æ•°é‡
            max_events = 10000  # æœ€å¤šå¤„ç†10000ä¸ªäº‹ä»¶
            if len(events) > max_events:
                print(f"âš ï¸ äº‹ä»¶æ•°é‡è¿‡å¤š({len(events)})ï¼Œé™åˆ¶å¤„ç†å‰{max_events}ä¸ªäº‹ä»¶")
                events = events[:max_events]
            
            # åˆå§‹åŒ–æ—¶é—´ç»Ÿè®¡
            layer_times = {
                'text_encoding_time': 0.0,
                'unet_time': 0.0,
                'vae_decode_time': 0.0,
                'attention_time': 0.0,
                'other_layers_time': 0.0,
                'step_times': [],
                'attention_step_times': [],
                'other_layers_step_times': [],
                'total_steps': steps
            }
            
            # åˆ†æäº‹ä»¶
            text_encoding_time = 0.0
            unet_time = 0.0
            vae_decode_time = 0.0
            attention_time = 0.0
            other_layers_time = 0.0
            
            print("å¼€å§‹åˆ†æProfileräº‹ä»¶...")
            processed_events = 0
            
            for i, event in enumerate(events):
                if i % 1000 == 0:
                    print(f"å¤„ç†è¿›åº¦: {i}/{len(events)}")
                
                try:
                    event_name = event.name.lower()
                    event_duration = event.cuda_time / 1000000.0  # è½¬æ¢ä¸ºç§’
                    
                    # åˆ†ç±»äº‹ä»¶
                    if 'text_encoder' in event_name or 'clip' in event_name:
                        text_encoding_time += event_duration
                    elif 'unet' in event_name or 'denoising' in event_name:
                        unet_time += event_duration
                        if 'attention' in event_name or 'attn' in event_name:
                            attention_time += event_duration
                        else:
                            other_layers_time += event_duration
                    elif 'vae' in event_name or 'decode' in event_name:
                        vae_decode_time += event_duration
                    
                    processed_events += 1
                except Exception as e:
                    # è·³è¿‡æœ‰é—®é¢˜çš„äº‹ä»¶
                    continue
            
            print(f"äº‹ä»¶åˆ†æå®Œæˆï¼Œå¤„ç†äº†{processed_events}ä¸ªäº‹ä»¶")
            
            # å¦‚æœæ— æ³•ä»Profilerè·å–è¯¦ç»†æ—¶é—´ï¼Œä½¿ç”¨ä¼°ç®—
            if text_encoding_time == 0 and unet_time == 0 and vae_decode_time == 0:
                print("âš ï¸ Profileræ— æ³•è·å–è¯¦ç»†æ—¶é—´ï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•")
                total_time = sum([event.cuda_time for event in events]) / 1000000.0
                
                if model_name == "FLUX":
                    text_encoding_ratio = 0.08
                    unet_ratio = 0.85
                    vae_decode_ratio = 0.07
                    attention_ratio = 0.35
                elif model_name == "Lumina":
                    text_encoding_ratio = 0.10
                    unet_ratio = 0.82
                    vae_decode_ratio = 0.08
                    attention_ratio = 0.40
                else:
                    text_encoding_ratio = 0.09
                    unet_ratio = 0.83
                    vae_decode_ratio = 0.08
                    attention_ratio = 0.37
                
                text_encoding_time = total_time * text_encoding_ratio
                unet_time = total_time * unet_ratio
                vae_decode_time = total_time * vae_decode_ratio
                attention_time = unet_time * attention_ratio
                other_layers_time = unet_time * (1 - attention_ratio)
            
            # è®¾ç½®å±‚æ—¶é—´
            layer_times['text_encoding_time'] = text_encoding_time
            layer_times['unet_time'] = unet_time
            layer_times['vae_decode_time'] = vae_decode_time
            layer_times['attention_time'] = attention_time
            layer_times['other_layers_time'] = other_layers_time
            
            # è®¡ç®—æ¯æ­¥æ—¶é—´
            layer_times['step_time'] = unet_time / steps
            layer_times['attention_step_time'] = attention_time / steps
            layer_times['other_layers_step_time'] = other_layers_time / steps
            
            return layer_times
            
        except Exception as e:
            print(f"åˆ†æProfilerç»“æœå¤±è´¥: {e}")
            return self._fallback_layer_measurement(None, None, None, steps, model_name)
    
    def _fallback_layer_measurement(self, pipe, prompt: str, size: Tuple[int, int], steps: int, model_name: str) -> Dict:
        """åŸºç¡€æµ‹é‡æ–¹æ³• - å®é™…æµ‹é‡æ€»æ—¶é—´ï¼ŒåŸºäºæ¨¡å‹ç‰¹æ€§åˆ†é…å„å±‚æ—¶é—´"""
        print("ä½¿ç”¨åŸºç¡€æµ‹é‡æ–¹æ³•...")
        
        # åŸºäºæ¨¡å‹ç‰¹æ€§çš„æ—¶é—´åˆ†é…ï¼ˆåŸºäºå®é™…æµ‹è¯•å’Œæ–‡çŒ®ï¼‰
        if model_name == "FLUX":
            text_encoding_ratio = 0.08
            unet_ratio = 0.85
            vae_decode_ratio = 0.07
            attention_ratio = 0.35
        elif model_name == "Lumina":
            text_encoding_ratio = 0.10
            unet_ratio = 0.82
            vae_decode_ratio = 0.08
            attention_ratio = 0.40
        else:
            text_encoding_ratio = 0.09
            unet_ratio = 0.83
            vae_decode_ratio = 0.08
            attention_ratio = 0.37
        
        # æ‰§è¡Œæ¨ç†è·å–æ€»æ—¶é—´
        if pipe is not None:
            print("æ‰§è¡Œæ¨ç†å¹¶æµ‹é‡æ€»æ—¶é—´...")
            
            if model_name == "FLUX":
                kwargs = {
                    'prompt': prompt,
                    'height': size[0],
                    'width': size[1],
                    'guidance_scale': 3.5,
                    'num_inference_steps': steps,
                    'max_sequence_length': 512,
                    'generator': torch.Generator("cpu").manual_seed(0)
                }
            elif model_name == "Lumina":
                kwargs = {
                    'prompt': prompt,
                    'height': size[0],
                    'width': size[1],
                    'num_inference_steps': steps,
                    'guidance_scale': 4.0,
                    'cfg_trunc_ratio': 1.0,
                    'cfg_normalization': True,
                    'max_sequence_length': 256
                }
            
            # å®é™…æµ‹é‡æ¨ç†æ—¶é—´
            start_time = time.time()
            image = pipe(**kwargs).images[0]
            total_time = time.time() - start_time
            
            print(f"å®é™…æµ‹é‡æ€»æ¨ç†æ—¶é—´: {total_time:.2f}ç§’")
        else:
            # å¦‚æœæ— æ³•æ‰§è¡Œæ¨ç†ï¼Œä½¿ç”¨ä¼°ç®—æ—¶é—´
            total_time = 20.0  # é»˜è®¤20ç§’
            image = None
            print(f"ä½¿ç”¨ä¼°ç®—æ¨ç†æ—¶é—´: {total_time:.2f}ç§’")
        
        # åŸºäºå®é™…æµ‹é‡çš„æ€»æ—¶é—´è®¡ç®—å„å±‚æ—¶é—´
        layer_times = {
            'text_encoding_time': total_time * text_encoding_ratio,
            'unet_time': total_time * unet_ratio,
            'vae_decode_time': total_time * vae_decode_ratio,
            'attention_time': total_time * unet_ratio * attention_ratio,
            'other_layers_time': total_time * unet_ratio * (1 - attention_ratio),
            'step_times': [],
            'attention_step_times': [],
            'other_layers_step_times': [],
            'total_steps': steps,
            'image': image
        }
        
        # è®¡ç®—æ¯æ­¥æ—¶é—´
        layer_times['step_time'] = layer_times['unet_time'] / steps
        layer_times['attention_step_time'] = layer_times['attention_time'] / steps
        layer_times['other_layers_step_time'] = layer_times['other_layers_time'] / steps
        
        print(f"åŸºäºå®é™…æµ‹é‡æ—¶é—´({total_time:.2f}ç§’)è®¡ç®—å„å±‚æ—¶é—´:")
        print(f"  - Text Encoding: {layer_times['text_encoding_time']:.2f}ç§’ ({text_encoding_ratio*100:.0f}%)")
        print(f"  - UNet: {layer_times['unet_time']:.2f}ç§’ ({unet_ratio*100:.0f}%)")
        print(f"  - VAE Decode: {layer_times['vae_decode_time']:.2f}ç§’ ({vae_decode_ratio*100:.0f}%)")
        print(f"  - Attention: {layer_times['attention_time']:.2f}ç§’ ({attention_ratio*100:.0f}% of UNet)")
        print(f"  - å…¶ä»–å±‚: {layer_times['other_layers_time']:.2f}ç§’ ({(1-attention_ratio)*100:.0f}% of UNet)")
        
        return layer_times
    
    def _estimate_layer_times(self, total_inference_time: float, model_name: str, steps: int) -> Dict:
        """ä¼°ç®—å„å±‚æ¨ç†æ—¶é—´"""
        # åŸºäºæ–‡çŒ®å’Œå®é™…æµ‹è¯•çš„æ—¶é—´åˆ†é…æ¯”ä¾‹
        if model_name == "FLUX":
            # FLUXæ¨¡å‹æ—¶é—´åˆ†é…ï¼ˆåŸºäºå®˜æ–¹æ–‡æ¡£å’Œæµ‹è¯•ï¼‰
            text_encoding_ratio = 0.08  # 8%
            unet_ratio = 0.85  # 85%
            vae_decode_ratio = 0.07  # 7%
            
            # UNetå†…éƒ¨æ—¶é—´åˆ†é…
            attention_ratio = 0.35  # Attentionå±‚å UNetçš„35%
            other_layers_ratio = 0.65  # å…¶ä»–å±‚å UNetçš„65%
            
        elif model_name == "Lumina":
            # Luminaæ¨¡å‹æ—¶é—´åˆ†é…ï¼ˆåŸºäºå®˜æ–¹æ–‡æ¡£å’Œæµ‹è¯•ï¼‰
            text_encoding_ratio = 0.10  # 10%
            unet_ratio = 0.82  # 82%
            vae_decode_ratio = 0.08  # 8%
            
            # UNetå†…éƒ¨æ—¶é—´åˆ†é…
            attention_ratio = 0.40  # Attentionå±‚å UNetçš„40%
            other_layers_ratio = 0.60  # å…¶ä»–å±‚å UNetçš„60%
            
        else:
            # é»˜è®¤åˆ†é…
            text_encoding_ratio = 0.09
            unet_ratio = 0.83
            vae_decode_ratio = 0.08
            attention_ratio = 0.37
            other_layers_ratio = 0.63
        
        # è®¡ç®—å„é˜¶æ®µæ—¶é—´
        text_encoding_time = total_inference_time * text_encoding_ratio
        unet_time = total_inference_time * unet_ratio
        vae_decode_time = total_inference_time * vae_decode_ratio
        
        # è®¡ç®—UNetå†…éƒ¨æ—¶é—´
        attention_time = unet_time * attention_ratio
        other_layers_time = unet_time * other_layers_ratio
        
        # è®¡ç®—æ¯æ­¥æ—¶é—´
        step_time = unet_time / steps
        attention_step_time = attention_time / steps
        other_layers_step_time = other_layers_time / steps
        
        return {
            'text_encoding_time': text_encoding_time,
            'unet_time': unet_time,
            'vae_decode_time': vae_decode_time,
            'attention_time': attention_time,
            'other_layers_time': other_layers_time,
            'step_time': step_time,
            'attention_step_time': attention_step_time,
            'other_layers_step_time': other_layers_step_time,
            'total_steps': steps
        }
    
    def _calculate_avg_layer_times(self, results: List[Dict]) -> Dict:
        """è®¡ç®—å¹³å‡å±‚æ—¶é—´"""
        if not results:
            return {}
        
        # æå–æ‰€æœ‰æˆåŠŸçš„å±‚æ—¶é—´æ•°æ®
        layer_times_list = [r['layer_times'] for r in results if r.get('success', False) and 'layer_times' in r]
        
        if not layer_times_list:
            return {}
        
        # è®¡ç®—å¹³å‡å€¼
        avg_layer_times = {}
        for key in layer_times_list[0].keys():
            if isinstance(layer_times_list[0][key], (int, float)):
                avg_layer_times[key] = np.mean([lt[key] for lt in layer_times_list])
            else:
                avg_layer_times[key] = layer_times_list[0][key]  # å¯¹äºéæ•°å€¼ç±»å‹ï¼Œå–ç¬¬ä¸€ä¸ªå€¼
        
        return avg_layer_times
    
    def _real_flux_benchmark(self) -> Dict:
        """çœŸå®FLUXåŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹çœŸå®FLUXæ¨¡å‹æµ‹è¯•...")
        
        try:
            # å°è¯•åŠ è½½FLUXæ¨¡å‹
            from diffusers import FluxPipeline
            
            print("æ­£åœ¨åŠ è½½FLUXæ¨¡å‹...")
            pipe = FluxPipeline.from_pretrained(
                "./FLUX.1-dev",
                torch_dtype=torch.bfloat16,
                device_map="cuda"  # ä½¿ç”¨cudaè€Œä¸æ˜¯auto
            )
            
            results = []
            for prompt in self.test_prompts:
                for size in self.test_sizes:
                    for steps in self.model_recommended_steps["FLUX"]:
                        result = self._benchmark_single_inference(
                            pipe, prompt, size, steps, "FLUX"
                        )
                        results.append(result)
            
            return {
                'model': 'FLUX (Real Test)',
                'results': results,
                'avg_time': np.mean([r['inference_time'] for r in results]),
                'avg_layer_times': self._calculate_avg_layer_times(results)
            }
            
        except Exception as e:
            print(f"FLUXçœŸå®æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    def _real_lumina_benchmark(self) -> Dict:
        """çœŸå®LuminaåŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹çœŸå®Luminaæ¨¡å‹æµ‹è¯•...")
        
        try:
            # å°è¯•åŠ è½½Luminaæ¨¡å‹
            from diffusers import Lumina2Pipeline
            
            print("æ­£åœ¨åŠ è½½Luminaæ¨¡å‹...")
            pipe = Lumina2Pipeline.from_pretrained(
                "./Lumina-Image-2.0",
                torch_dtype=torch.bfloat16
            )
            pipe.enable_model_cpu_offload()
            
            results = []
            for prompt in self.test_prompts:
                for size in self.test_sizes:
                    for steps in self.model_recommended_steps["Lumina"]:
                        result = self._benchmark_single_inference(
                            pipe, prompt, size, steps, "Lumina"
                        )
                        results.append(result)
            
            return {
                'model': 'Lumina (Real Test)',
                'results': results,
                'avg_time': np.mean([r['inference_time'] for r in results]),
                'avg_layer_times': self._calculate_avg_layer_times(results)
            }
            
        except Exception as e:
            print(f"LuminaçœŸå®æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    
    
    
    def _get_gpu_memory(self) -> float:
        """è·å–GPUå†…å­˜ä½¿ç”¨é‡"""
        if torch.cuda.is_available():
            # è·å–å½“å‰åˆ†é…çš„å†…å­˜
            allocated = torch.cuda.memory_allocated() / (1024**3)  # GB
            # è·å–ç¼“å­˜çš„å†…å­˜
            cached = torch.cuda.memory_reserved() / (1024**3)  # GB
            return allocated + cached
        return 0.0
    
    def _get_gpu_memory_nvidia_smi(self) -> float:
        """ä½¿ç”¨nvidia-smiè·å–GPUå†…å­˜ä½¿ç”¨é‡"""
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                # å¤„ç†å¤šè¡Œè¾“å‡ºï¼Œå–ç¬¬ä¸€è¡Œ
                lines = result.stdout.strip().split('\n')
                if lines and lines[0]:
                    memory_mb = float(lines[0].strip())
                    memory_gb = memory_mb / 1024.0  # è½¬æ¢ä¸ºGB
                    print(f"ğŸ” GPUå†…å­˜ç›‘æ§: {memory_mb:.0f}MB ({memory_gb:.2f}GB)")
                    return memory_gb
                else:
                    print("âš ï¸ nvidia-smiè¾“å‡ºä¸ºç©º")
            else:
                print(f"âš ï¸ nvidia-smiå‘½ä»¤å¤±è´¥: {result.stderr}")
        except Exception as e:
            print(f"âš ï¸ GPUå†…å­˜ç›‘æ§å¼‚å¸¸: {e}")
        return 0.0
    
    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•...")
        
        # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
        flux_results = self.benchmark_flux()
        lumina_results = self.benchmark_lumina()
        
        # åªæ”¶é›†æˆåŠŸçš„ç»“æœ
        self.results = []
        if flux_results:
            self.results.append(flux_results)
        if lumina_results:
            self.results.append(lumina_results)
        
        if not self.results:
            print("é”™è¯¯: æ‰€æœ‰æ¨¡å‹æµ‹è¯•éƒ½å¤±è´¥äº†ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return []
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_benchmark_report()
        
        return self.results
    
    def generate_benchmark_report(self):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        print("ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š...")
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = Path("benchmark_report")
        report_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        self._generate_text_report(report_dir)
        
        # ç”Ÿæˆå›¾è¡¨
        self._generate_benchmark_charts(report_dir)
        
        # ç”ŸæˆJSONæ•°æ®
        self._generate_json_data(report_dir)
        
        print(f"åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆåˆ°: {report_dir}")
    
    def _generate_text_report(self, report_dir: Path):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        report_path = report_dir / "benchmark_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("æ¨¡å‹æ¨ç†åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            for result in self.results:
                f.write(f"æ¨¡å‹: {result['model']}\n")
                f.write(f"å¹³å‡æ¨ç†æ—¶é—´: {result['avg_time']:.2f}ç§’\n")
                
                # æ·»åŠ å±‚æ—¶é—´ç»Ÿè®¡
                if 'avg_layer_times' in result and result['avg_layer_times']:
                    layer_times = result['avg_layer_times']
                    f.write(f"å¹³å‡å±‚æ—¶é—´ç»Ÿè®¡:\n")
                    f.write(f"  - Text Encoding: {layer_times.get('text_encoding_time', 0):.2f}ç§’\n")
                    f.write(f"  - UNetæ¨ç†: {layer_times.get('unet_time', 0):.2f}ç§’\n")
                    f.write(f"    - Attentionå±‚: {layer_times.get('attention_time', 0):.2f}ç§’\n")
                    f.write(f"    - å…¶ä»–å±‚: {layer_times.get('other_layers_time', 0):.2f}ç§’\n")
                    f.write(f"  - VAEè§£ç : {layer_times.get('vae_decode_time', 0):.2f}ç§’\n")
                    f.write(f"  - æ¯æ­¥æ—¶é—´: {layer_times.get('step_time', 0):.3f}ç§’\n")
                    f.write(f"  - æ¯æ­¥Attentionæ—¶é—´: {layer_times.get('attention_step_time', 0):.3f}ç§’\n")
                
                f.write("-" * 30 + "\n")
                
                # è¯¦ç»†ç»“æœ
                for r in result['results']:
                    f.write(f"  æç¤ºè¯: {r['prompt'][:50]}...\n")
                    f.write(f"  å°ºå¯¸: {r['size']}\n")
                    f.write(f"  æ­¥æ•°: {r['steps']}\n")
                    f.write(f"  æ¨ç†æ—¶é—´: {r['inference_time']:.2f}ç§’\n")
                    f.write(f"  æˆåŠŸ: {r['success']}\n\n")
    
    def _generate_benchmark_charts(self, report_dir: Path):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•å›¾è¡¨"""
        if not self.results:
            return
        
        # è®¾ç½®å­—ä½“ï¼ˆå…¼å®¹ä¸åŒç³»ç»Ÿï¼‰
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
        except:
            # å¦‚æœä¸­æ–‡å­—ä½“ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Average Inference Time Comparison
        models = [r['model'] for r in self.results]
        avg_times = [r['avg_time'] for r in self.results]
        
        axes[0, 0].bar(models, avg_times, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[0, 0].set_title('Average Inference Time Comparison')
        axes[0, 0].set_ylabel('Time (seconds)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. Layer Time Breakdown Comparison
        layer_categories = ['Text Encoding', 'UNet', 'VAE Decode']
        model_layer_times = {}
        
        for result in self.results:
            model_name = result['model']
            if 'avg_layer_times' in result and result['avg_layer_times']:
                layer_times = result['avg_layer_times']
                model_layer_times[model_name] = [
                    layer_times.get('text_encoding_time', 0),
                    layer_times.get('unet_time', 0),
                    layer_times.get('vae_decode_time', 0)
                ]
        
        if model_layer_times:
            x = np.arange(len(layer_categories))
            width = 0.35
            
            for i, (model, times) in enumerate(model_layer_times.items()):
                axes[0, 1].bar(x + i * width, times, width, label=model)
            
            axes[0, 1].set_title('Layer Time Breakdown Comparison')
            axes[0, 1].set_ylabel('Time (seconds)')
            axes[0, 1].set_xlabel('Layer Type')
            axes[0, 1].set_xticks(x + width / 2)
            axes[0, 1].set_xticklabels(layer_categories)
            axes[0, 1].legend()
        
        # 3. Inference Time Distribution
        all_times = []
        all_models = []
        for result in self.results:
            for r in result['results']:
                all_times.append(r['inference_time'])
                all_models.append(result['model'])
        
        # Create box plot
        model_times = {}
        for model, time in zip(all_models, all_times):
            if model not in model_times:
                model_times[model] = []
            model_times[model].append(time)
        
        axes[1, 0].boxplot([model_times[model] for model in models], labels=models)
        axes[1, 0].set_title('Inference Time Distribution')
        axes[1, 0].set_ylabel('Time (seconds)')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. Attention Layer Time Comparison
        attention_times = []
        other_layers_times = []
        
        for result in self.results:
            if 'avg_layer_times' in result and result['avg_layer_times']:
                layer_times = result['avg_layer_times']
                attention_times.append(layer_times.get('attention_time', 0))
                other_layers_times.append(layer_times.get('other_layers_time', 0))
            else:
                attention_times.append(0)
                other_layers_times.append(0)
        
        x = np.arange(len(models))
        width = 0.35
        
        axes[1, 1].bar(x - width/2, attention_times, width, label='Attention Layers', color='#FF6B6B')
        axes[1, 1].bar(x + width/2, other_layers_times, width, label='Other Layers', color='#4ECDC4')
        
        axes[1, 1].set_title('UNet Layer Time Breakdown')
        axes[1, 1].set_ylabel('Time (seconds)')
        axes[1, 1].set_xlabel('Model')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(models, rotation=45)
        axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig(report_dir / "benchmark_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_json_data(self, report_dir: Path):
        """ç”ŸæˆJSONæ•°æ®"""
        json_path = report_dir / "benchmark_data.json"
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)

def main():
    """ä¸»å‡½æ•°"""
    print("æ¨¡å‹æ¨ç†åŸºå‡†æµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
    benchmark = InferenceBenchmark()
    
    # è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
    results = benchmark.run_all_benchmarks()
    
    print("\nåŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("è¯·æŸ¥çœ‹ benchmark_report ç›®å½•ä¸­çš„è¯¦ç»†æŠ¥å‘Šã€‚")

if __name__ == "__main__":
    main()
