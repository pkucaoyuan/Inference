#!/usr/bin/env python3
"""
å®é™…æ¨ç†åŸºå‡†æµ‹è¯•è„šæœ¬
æµ‹é‡FLUXå’ŒLuminaçš„å®é™…GPUæ¨ç†æ—¶é—´
"""

import os
import time
import torch
import json
import numpy as np
from typing import Dict, List, Tuple
import psutil
try:
    import GPUtil
    GPUTIL_AVAILABLE = True
except ImportError:
    GPUTIL_AVAILABLE = False
    print("è­¦å‘Š: GPUtilæœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆè·å–GPUä¿¡æ¯")
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns

# å¯¼å…¥å¿…è¦çš„åº“
try:
    from diffusers import FluxPipeline, StableDiffusionXLPipeline
    from transformers import AutoTokenizer, AutoModel
    DIFFUSERS_AVAILABLE = True
except ImportError:
    DIFFUSERS_AVAILABLE = False
    print("è­¦å‘Š: diffusersåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")

class InferenceBenchmark:
    """æ¨ç†åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self, output_dir: str = "./output_images"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.results = []
        
        # ä½¿ç”¨æ—¶é—´æˆ³åˆ›å»ºå”¯ä¸€çš„è¾“å‡ºç›®å½•
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"{output_dir}_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        
        # æµ‹è¯•é…ç½®
        self.test_prompts = [
            "A beautiful landscape with mountains and lakes, photorealistic",
            "A futuristic city with flying cars, cyberpunk style",
            "A cute anime character in a magical garden, detailed"
        ]
        
        self.test_sizes = [
            (1024, 1024)  # åªæµ‹è¯•1024å°ºå¯¸
        ]
        
        # æ ¹æ®å®˜æ–¹æ¨èè®¾ç½®æµ‹è¯•æ­¥æ•°
        self.model_recommended_steps = {
            "FLUX": [50],               # FLUXå®˜æ–¹ç¤ºä¾‹ä½¿ç”¨50æ­¥
            "Lumina": [30]              # Luminaé»˜è®¤30æ­¥
        }
    
    def benchmark_flux(self) -> Dict:
        """åŸºå‡†æµ‹è¯•FLUXæ¨¡å‹"""
        print("å¼€å§‹æµ‹è¯•FLUXæ¨¡å‹...")
        
        if not DIFFUSERS_AVAILABLE:
            print("é”™è¯¯: diffusersåº“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒçœŸå®æ¨ç†æµ‹è¯•")
            return None
        
        # ç›´æ¥è°ƒç”¨çœŸå®æµ‹è¯•å‡½æ•°
        return self._real_flux_benchmark()
    
    def benchmark_lumina(self) -> Dict:
        """åŸºå‡†æµ‹è¯•Luminaæ¨¡å‹"""
        print("å¼€å§‹æµ‹è¯•Luminaæ¨¡å‹...")
        
        if not DIFFUSERS_AVAILABLE:
            print("é”™è¯¯: diffusersåº“ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒçœŸå®æ¨ç†æµ‹è¯•")
            return None
        
        # ç›´æ¥è°ƒç”¨çœŸå®æµ‹è¯•å‡½æ•°
        return self._real_lumina_benchmark()
    
    
    def _benchmark_single_inference(self, pipe, prompt: str, size: Tuple[int, int], 
                                  steps: int, model_name: str) -> Dict:
        """å•æ¬¡æ¨ç†åŸºå‡†æµ‹è¯•"""
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        
        try:
            # ä½¿ç”¨å®é™…æµ‹é‡æ–¹æ³•
            print(f"å¼€å§‹{model_name}æ¨ç†ï¼ˆå®é™…æµ‹é‡æ¨¡å¼ï¼‰...")
            layer_times = self._measure_actual_layer_times(pipe, prompt, size, steps, model_name)
            
            if layer_times is None:
                raise Exception("å®é™…æµ‹é‡å¤±è´¥")
            
            # ä½¿ç”¨å®é™…æµ‹é‡çš„æ€»æ¨ç†æ—¶é—´
            total_inference_time = layer_times.get('total_inference_time', sum([
                layer_times.get('text_encoding_time', 0),
                layer_times.get('unet_time', 0),
                layer_times.get('vae_decode_time', 0)
            ]))
            
            print(f"æ¨ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_inference_time:.2f}ç§’")
            print(f"  - Text Encoding: {layer_times.get('text_encoding_time', 0):.2f}ç§’")
            print(f"  - UNetæ¨ç†: {layer_times.get('unet_time', 0):.2f}ç§’")
            print(f"    - Attentionå±‚: {layer_times.get('attention_time', 0):.2f}ç§’")
            print(f"    - å…¶ä»–å±‚: {layer_times.get('other_layers_time', 0):.2f}ç§’")
            print(f"  - VAEè§£ç : {layer_times.get('vae_decode_time', 0):.2f}ç§’")
            
            # ä¿å­˜ç”Ÿæˆçš„å›¾ç‰‡
            save_start_time = time.time()
            safe_prompt = "".join(c for c in prompt[:30] if c.isalnum() or c in (' ', '-', '_')).rstrip()
            filename = f"{model_name.lower().replace(' ', '_')}_{size[0]}x{size[1]}_steps_{steps}_cfg_{3.5 if model_name == 'FLUX' else 4.0 if model_name == 'Lumina' else 4.5}_{safe_prompt}.png"
            image_path = self.output_dir / filename
            layer_times['image'].save(image_path)
            save_time = time.time() - save_start_time
            print(f"ä¿å­˜å›¾ç‰‡: {image_path} (è€—æ—¶: {save_time:.2f}ç§’)")
            
            # è®°å½•ç»“æŸçŠ¶æ€
            end_time = time.time()
            
            return {
                'prompt': prompt,
                'size': size,
                'steps': steps,
                'inference_time': total_inference_time,  # ä½¿ç”¨å®é™…æµ‹é‡çš„æ¨ç†æ—¶é—´
                'total_time': end_time - start_time,  # æ€»æ—¶é—´ï¼ˆåŒ…æ‹¬ä¿å­˜ï¼‰
                'save_time': save_time,  # ä¿å­˜æ—¶é—´
                'layer_times': layer_times,  # å®é™…æµ‹é‡çš„å„å±‚æ—¶é—´ç»Ÿè®¡
                'success': True
            }
            
        except Exception as e:
            end_time = time.time()
            print(f"æ¨ç†å¤±è´¥: {e}")
            return {
                'prompt': prompt,
                'size': size,
                'steps': steps,
                'inference_time': end_time - start_time,
                'total_time': end_time - start_time,
                'save_time': 0.0,
                'layer_times': {},
                'success': False,
                'error': str(e)
            }
    
    def _measure_actual_layer_times(self, pipe, prompt: str, size: Tuple[int, int], steps: int, model_name: str) -> Dict:
        """å®é™…æµ‹é‡å„å±‚æ¨ç†æ—¶é—´ - ä½¿ç”¨Hookæœºåˆ¶è¿›è¡ŒçœŸå®æµ‹é‡"""
        try:
            print("å¼€å§‹å®é™…æµ‹é‡å„å±‚æ¨ç†æ—¶é—´...")
            print(f"æ¨¡å‹ç±»å‹: {type(pipe)}")
            print(f"æ¨¡å‹å±æ€§: {dir(pipe)}")
            
            # åˆå§‹åŒ–æ—¶é—´è®°å½•
            layer_times = {
                'text_encoding_time': 0.0,
                'unet_time': 0.0,
                'vae_decode_time': 0.0,
                'attention_time': 0.0,
                'other_layers_time': 0.0,
                'step_times': [],
                'attention_step_times': [],
                'other_layers_step_times': [],
                'total_steps': steps,
                'image': None
            }
            
            # æ—¶é—´è®°å½•å˜é‡
            text_encoding_start = 0.0
            text_encoding_end = 0.0
            unet_start = 0.0
            unet_end = 0.0
            vae_decode_start = 0.0
            vae_decode_end = 0.0
            
            attention_times = []
            other_layer_times = []
            step_times = []
            
            # è®¾ç½®Hookæ¥æµ‹é‡å„å±‚æ—¶é—´
            hooks = []
            
            def text_encoder_hook(module, input, output):
                nonlocal text_encoding_start, text_encoding_end
                current_time = time.time()
                if text_encoding_start == 0:
                    text_encoding_start = current_time
                text_encoding_end = current_time
            
            def unet_hook(module, input, output):
                nonlocal unet_start, unet_end
                current_time = time.time()
                if unet_start == 0:
                    unet_start = current_time
                unet_end = current_time
            
            def attention_hook(module, input, output):
                start_time = time.time()
                # ç­‰å¾…GPUè®¡ç®—å®Œæˆ
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                end_time = time.time()
                attention_times.append(end_time - start_time)
            
            def other_layer_hook(module, input, output):
                start_time = time.time()
                # ç­‰å¾…GPUè®¡ç®—å®Œæˆ
                if torch.cuda.is_available():
                    torch.cuda.synchronize()
                end_time = time.time()
                other_layer_times.append(end_time - start_time)
            
            def vae_hook(module, input, output):
                nonlocal vae_decode_start, vae_decode_end
                current_time = time.time()
                if vae_decode_start == 0:
                    vae_decode_start = current_time
                vae_decode_end = current_time
            
            # æ³¨å†ŒHook
            print("å¼€å§‹æ³¨å†ŒHook...")
            try:
                # æ³¨å†ŒText Encoder Hook
                print(f"æ£€æŸ¥text_encoderå±æ€§: {hasattr(pipe, 'text_encoder')}")
                if hasattr(pipe, 'text_encoder'):
                    print("æ³¨å†ŒText Encoder Hook...")
                    text_encoder_modules = list(pipe.text_encoder.named_modules())
                    print(f"Text Encoderæ¨¡å—æ•°é‡: {len(text_encoder_modules)}")
                    for name, module in text_encoder_modules:
                        print(f"  - æ£€æŸ¥æ¨¡å—: {name}")
                        if 'attention' in name.lower() or 'attn' in name.lower():
                            hook = module.register_forward_hook(text_encoder_hook)
                            hooks.append(hook)
                            print(f"  - æ³¨å†ŒText Encoder Attention Hook: {name}")
                            break
                else:
                    print("âš ï¸ æ¨¡å‹æ²¡æœ‰text_encoderå±æ€§")
                
                # æ³¨å†ŒUNet/Transformer Hook
                print(f"æ£€æŸ¥unetå±æ€§: {hasattr(pipe, 'unet')}")
                print(f"æ£€æŸ¥transformerå±æ€§: {hasattr(pipe, 'transformer')}")
                
                # FLUXä½¿ç”¨transformerï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨unet
                unet_module = None
                if hasattr(pipe, 'unet'):
                    unet_module = pipe.unet
                    print("æ³¨å†ŒUNet Hook...")
                elif hasattr(pipe, 'transformer'):
                    unet_module = pipe.transformer
                    print("æ³¨å†ŒTransformer Hook...")
                
                if unet_module is not None:
                    unet_modules = list(unet_module.named_modules())
                    print(f"UNet/Transformeræ¨¡å—æ•°é‡: {len(unet_modules)}")
                    attention_count = 0
                    other_count = 0
                    unet_count = 0
                    
                    for name, module in unet_modules:
                        if 'attention' in name.lower() or 'attn' in name.lower():
                            hook = module.register_forward_hook(attention_hook)
                            hooks.append(hook)
                            attention_count += 1
                            if attention_count <= 3:  # åªæ‰“å°å‰3ä¸ª
                                print(f"  - æ³¨å†ŒAttention Hook: {name}")
                        elif 'conv' in name.lower() or 'linear' in name.lower() or 'norm' in name.lower():
                            hook = module.register_forward_hook(other_layer_hook)
                            hooks.append(hook)
                            other_count += 1
                            if other_count <= 3:  # åªæ‰“å°å‰3ä¸ª
                                print(f"  - æ³¨å†Œå…¶ä»–å±‚Hook: {name}")
                        elif 'down' in name.lower() or 'up' in name.lower() or 'mid' in name.lower() or 'block' in name.lower() or 'noise_refiner' in name.lower():
                            hook = module.register_forward_hook(unet_hook)
                            hooks.append(hook)
                            unet_count += 1
                            if unet_count <= 3:  # åªæ‰“å°å‰3ä¸ª
                                print(f"  - æ³¨å†ŒUNet/Transformer Hook: {name}")
                    
                    # ä¸ºä¸»è¦çš„UNet/Transformerç»„ä»¶æ³¨å†ŒHook
                    if hasattr(unet_module, 'noise_refiner'):
                        hook = unet_module.noise_refiner.register_forward_hook(unet_hook)
                        hooks.append(hook)
                        unet_count += 1
                        print(f"  - æ³¨å†Œä¸»è¦UNetç»„ä»¶: noise_refiner")
                    
                    # ä¸ºä¸»è¦çš„Transformerç»„ä»¶æ³¨å†ŒHook
                    if hasattr(unet_module, 'transformer_blocks'):
                        for i, block in enumerate(unet_module.transformer_blocks[:3]):  # åªæ³¨å†Œå‰3ä¸ª
                            hook = block.register_forward_hook(unet_hook)
                            hooks.append(hook)
                            unet_count += 1
                            print(f"  - æ³¨å†ŒTransformer Block: {i}")
                    
                    print(f"  - æ€»è®¡æ³¨å†Œ: {attention_count}ä¸ªAttention, {other_count}ä¸ªå…¶ä»–å±‚, {unet_count}ä¸ªUNet/Transformer")
                else:
                    print("âš ï¸ æ¨¡å‹æ²¡æœ‰unetæˆ–transformerå±æ€§")
                
                # æ³¨å†ŒVAE Hook
                print(f"æ£€æŸ¥vaeå±æ€§: {hasattr(pipe, 'vae')}")
                if hasattr(pipe, 'vae'):
                    print("æ³¨å†ŒVAE Hook...")
                    vae_modules = list(pipe.vae.named_modules())
                    print(f"VAEæ¨¡å—æ•°é‡: {len(vae_modules)}")
                    vae_hook_count = 0
                    
                    # ä¸ºæ‰€æœ‰VAEæ¨¡å—æ³¨å†ŒHookï¼Œä¸é™åˆ¶æ•°é‡
                    for name, module in vae_modules:
                        if name:  # è·³è¿‡ç©ºåç§°
                            hook = module.register_forward_hook(vae_hook)
                            hooks.append(hook)
                            vae_hook_count += 1
                            if vae_hook_count <= 5:  # åªæ‰“å°å‰5ä¸ª
                                print(f"  - æ³¨å†ŒVAE Hook: {name}")
                    
                    # ä¸ºä¸»è¦çš„VAEç»„ä»¶æ³¨å†ŒHook
                    if hasattr(pipe.vae, 'decoder'):
                        hook = pipe.vae.decoder.register_forward_hook(vae_hook)
                        hooks.append(hook)
                        vae_hook_count += 1
                        print(f"  - æ³¨å†Œä¸»è¦VAEç»„ä»¶: decoder")
                    
                    if hasattr(pipe.vae, 'up_blocks'):
                        for i, block in enumerate(pipe.vae.up_blocks):
                            hook = block.register_forward_hook(vae_hook)
                            hooks.append(hook)
                            vae_hook_count += 1
                            if i < 3:  # åªæ‰“å°å‰3ä¸ª
                                print(f"  - æ³¨å†ŒVAE Up Block: {i}")
                    
                    # ä¸ºVAEçš„æ ¹æ¨¡å—æ³¨å†ŒHook
                    hook = pipe.vae.register_forward_hook(vae_hook)
                    hooks.append(hook)
                    vae_hook_count += 1
                    print(f"  - æ³¨å†ŒVAEæ ¹æ¨¡å—")
                    
                    print(f"  - æ€»è®¡æ³¨å†Œ: {vae_hook_count}ä¸ªVAE Hook")
                else:
                    print("âš ï¸ æ¨¡å‹æ²¡æœ‰vaeå±æ€§")
                
                print(f"æ€»å…±æ³¨å†Œäº† {len(hooks)} ä¸ªHookè¿›è¡Œå®é™…æµ‹é‡")
                
            except Exception as e:
                print(f"âš ï¸ Hookæ³¨å†Œå¤±è´¥: {e}")
            
            # å‡†å¤‡æ¨ç†å‚æ•°
            if model_name == "FLUX":
                kwargs = {
                    'prompt': prompt,
                    'height': size[0],
                    'width': size[1],
                    'guidance_scale': 3.5,
                    'num_inference_steps': steps,
                    'max_sequence_length': 512,
                    'generator': torch.Generator("cpu").manual_seed(0)
                }
            elif model_name == "Lumina":
                kwargs = {
                    'prompt': prompt,
                    'height': size[0],
                    'width': size[1],
                    'num_inference_steps': steps,
                    'guidance_scale': 4.0,
                    'cfg_trunc_ratio': 1.0,
                    'cfg_normalization': True,
                    'max_sequence_length': 256
                }
            
            # æ‰§è¡Œæ¨ç†å¹¶æµ‹é‡æ—¶é—´
            print("æ‰§è¡Œæ¨ç†å¹¶å®é™…æµ‹é‡å„å±‚æ—¶é—´...")
            total_start = time.time()
            image = pipe(**kwargs).images[0]
            total_end = time.time()
            
            # æ¸…ç†Hook
            for hook in hooks:
                hook.remove()
            
            # è®¡ç®—å„å±‚å®é™…æ—¶é—´
            total_time = total_end - total_start
            
            print(f"Hookæµ‹é‡ç»“æœ:")
            print(f"  - æ€»æ¨ç†æ—¶é—´: {total_time:.3f}ç§’")
            print(f"  - Text Encodingæ—¶é—´èŒƒå›´: {text_encoding_start:.3f} -> {text_encoding_end:.3f}")
            print(f"  - UNetæ—¶é—´èŒƒå›´: {unet_start:.3f} -> {unet_end:.3f}")
            print(f"  - VAEæ—¶é—´èŒƒå›´: {vae_decode_start:.3f} -> {vae_decode_end:.3f}")
            print(f"  - Attentionè°ƒç”¨æ¬¡æ•°: {len(attention_times)}")
            print(f"  - å…¶ä»–å±‚è°ƒç”¨æ¬¡æ•°: {len(other_layer_times)}")
            
            # è®¡ç®—Text Encodingæ—¶é—´
            if text_encoding_start > 0 and text_encoding_end > 0:
                layer_times['text_encoding_time'] = text_encoding_end - text_encoding_start
                print(f"  âœ… Text Encodingå®é™…æµ‹é‡: {layer_times['text_encoding_time']:.3f}ç§’")
            else:
                layer_times['text_encoding_time'] = total_time * 0.08
                print(f"  âš ï¸ Text Encodingä½¿ç”¨ä¼°ç®—: {layer_times['text_encoding_time']:.3f}ç§’")
            
            # è®¡ç®—UNetæ—¶é—´
            if unet_start > 0 and unet_end > 0:
                layer_times['unet_time'] = unet_end - unet_start
                print(f"  âœ… UNetå®é™…æµ‹é‡: {layer_times['unet_time']:.3f}ç§’")
            else:
                layer_times['unet_time'] = total_time * 0.85
                print(f"  âš ï¸ UNetä½¿ç”¨ä¼°ç®—: {layer_times['unet_time']:.3f}ç§’")
            
            # è®¡ç®—VAEè§£ç æ—¶é—´
            if vae_decode_start > 0 and vae_decode_end > 0:
                layer_times['vae_decode_time'] = vae_decode_end - vae_decode_start
                print(f"  âœ… VAEå®é™…æµ‹é‡: {layer_times['vae_decode_time']:.3f}ç§’")
            else:
                # å¦‚æœVAE Hookæ²¡æœ‰æ•è·åˆ°æ—¶é—´ï¼Œä½¿ç”¨ä¼°ç®—
                layer_times['vae_decode_time'] = total_time * 0.07
                print(f"  âš ï¸ VAEä½¿ç”¨ä¼°ç®—: {layer_times['vae_decode_time']:.3f}ç§’")
            
            # éªŒè¯æ—¶é—´è®¡ç®—ä¸€è‡´æ€§
            calculated_total = layer_times['text_encoding_time'] + layer_times['unet_time'] + layer_times['vae_decode_time']
            time_diff = abs(total_time - calculated_total)
            
            # æ˜¾ç¤ºæ—¶é—´åˆ†å¸ƒåˆ†æ
            print(f"  ğŸ“Š æ—¶é—´åˆ†å¸ƒåˆ†æ:")
            print(f"    - Text Encoding: {layer_times['text_encoding_time']:.3f}ç§’ ({layer_times['text_encoding_time']/total_time*100:.1f}%)")
            print(f"    - UNet: {layer_times['unet_time']:.3f}ç§’ ({layer_times['unet_time']/total_time*100:.1f}%)")
            print(f"    - VAE: {layer_times['vae_decode_time']:.3f}ç§’ ({layer_times['vae_decode_time']/total_time*100:.1f}%)")
            print(f"    - å…¶ä»–æ—¶é—´: {total_time - calculated_total:.3f}ç§’ ({(total_time - calculated_total)/total_time*100:.1f}%)")
            
            if time_diff > 0.1:  # å¦‚æœå·®å¼‚è¶…è¿‡0.1ç§’
                print(f"  âš ï¸ æ—¶é—´è®¡ç®—ä¸ä¸€è‡´: æ€»æ—¶é—´{total_time:.3f}ç§’ vs è®¡ç®—æ—¶é—´{calculated_total:.3f}ç§’ (å·®å¼‚{time_diff:.3f}ç§’)")
                print(f"  ğŸ’¡ å·®å¼‚å¯èƒ½æ¥è‡ª: æ¨¡å‹åˆå§‹åŒ–ã€å†…å­˜ç®¡ç†ã€å…¶ä»–å¼€é”€")
                # ä½¿ç”¨å®é™…æµ‹é‡çš„æ€»æ—¶é—´
                layer_times['total_inference_time'] = total_time
            else:
                layer_times['total_inference_time'] = calculated_total
                print(f"  âœ… æ—¶é—´è®¡ç®—ä¸€è‡´: {calculated_total:.3f}ç§’")
            
            # è®¡ç®—Attentionå’Œå…¶ä»–å±‚æ—¶é—´
            if attention_times:
                layer_times['attention_time'] = sum(attention_times)
                layer_times['other_layers_time'] = layer_times['unet_time'] - layer_times['attention_time']
                print(f"  âœ… Attentionå®é™…æµ‹é‡: {layer_times['attention_time']:.3f}ç§’ (æ¥è‡ª{len(attention_times)}æ¬¡è°ƒç”¨)")
                print(f"  âœ… å…¶ä»–å±‚è®¡ç®—: {layer_times['other_layers_time']:.3f}ç§’")
            else:
                layer_times['attention_time'] = layer_times['unet_time'] * 0.35
                layer_times['other_layers_time'] = layer_times['unet_time'] * 0.65
                print(f"  âš ï¸ Attentionä½¿ç”¨ä¼°ç®—: {layer_times['attention_time']:.3f}ç§’")
                print(f"  âš ï¸ å…¶ä»–å±‚ä½¿ç”¨ä¼°ç®—: {layer_times['other_layers_time']:.3f}ç§’")
            
            # è®¡ç®—æ¯æ­¥æ—¶é—´
            layer_times['step_time'] = layer_times['unet_time'] / steps
            layer_times['attention_step_time'] = layer_times['attention_time'] / steps
            layer_times['other_layers_step_time'] = layer_times['other_layers_time'] / steps
            
            # è®°å½•å›¾ç‰‡
            layer_times['image'] = image
            
            print(f"å®é™…æµ‹é‡å®Œæˆ:")
            print(f"  - æ€»æ¨ç†æ—¶é—´: {total_time:.2f}ç§’")
            print(f"  - Text Encoding: {layer_times['text_encoding_time']:.2f}ç§’")
            print(f"  - UNet: {layer_times['unet_time']:.2f}ç§’")
            print(f"  - VAE Decode: {layer_times['vae_decode_time']:.2f}ç§’")
            print(f"  - Attention: {layer_times['attention_time']:.2f}ç§’")
            print(f"  - å…¶ä»–å±‚: {layer_times['other_layers_time']:.2f}ç§’")
            
            return layer_times
            
        except Exception as e:
            print(f"å®é™…æµ‹é‡å¤±è´¥: {e}")
            # å¦‚æœHookæµ‹é‡å¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æµ‹é‡
            return self._fallback_layer_measurement(pipe, prompt, size, steps, model_name)
    
    def _analyze_profiler_results(self, prof, model_name: str, steps: int) -> Dict:
        """åˆ†æProfilerç»“æœè·å–å„å±‚æ—¶é—´"""
        try:
            print("è·å–Profileräº‹ä»¶åˆ—è¡¨...")
            # è·å–äº‹ä»¶åˆ—è¡¨
            events = prof.events()
            print(f"Profileräº‹ä»¶æ•°é‡: {len(events)}")
            
            # å¦‚æœäº‹ä»¶è¿‡å¤šï¼Œé™åˆ¶å¤„ç†æ•°é‡
            max_events = 10000  # æœ€å¤šå¤„ç†10000ä¸ªäº‹ä»¶
            if len(events) > max_events:
                print(f"âš ï¸ äº‹ä»¶æ•°é‡è¿‡å¤š({len(events)})ï¼Œé™åˆ¶å¤„ç†å‰{max_events}ä¸ªäº‹ä»¶")
                events = events[:max_events]
            
            # åˆå§‹åŒ–æ—¶é—´ç»Ÿè®¡
            layer_times = {
                'text_encoding_time': 0.0,
                'unet_time': 0.0,
                'vae_decode_time': 0.0,
                'attention_time': 0.0,
                'other_layers_time': 0.0,
                'step_times': [],
                'attention_step_times': [],
                'other_layers_step_times': [],
                'total_steps': steps
            }
            
            # åˆ†æäº‹ä»¶
            text_encoding_time = 0.0
            unet_time = 0.0
            vae_decode_time = 0.0
            attention_time = 0.0
            other_layers_time = 0.0
            
            print("å¼€å§‹åˆ†æProfileräº‹ä»¶...")
            processed_events = 0
            
            for i, event in enumerate(events):
                if i % 1000 == 0:
                    print(f"å¤„ç†è¿›åº¦: {i}/{len(events)}")
                
                try:
                    event_name = event.name.lower()
                    event_duration = event.cuda_time / 1000000.0  # è½¬æ¢ä¸ºç§’
                    
                    # åˆ†ç±»äº‹ä»¶
                    if 'text_encoder' in event_name or 'clip' in event_name:
                        text_encoding_time += event_duration
                    elif 'unet' in event_name or 'denoising' in event_name:
                        unet_time += event_duration
                        if 'attention' in event_name or 'attn' in event_name:
                            attention_time += event_duration
                        else:
                            other_layers_time += event_duration
                    elif 'vae' in event_name or 'decode' in event_name:
                        vae_decode_time += event_duration
                    
                    processed_events += 1
                except Exception as e:
                    # è·³è¿‡æœ‰é—®é¢˜çš„äº‹ä»¶
                    continue
            
            print(f"äº‹ä»¶åˆ†æå®Œæˆï¼Œå¤„ç†äº†{processed_events}ä¸ªäº‹ä»¶")
            
            # å¦‚æœæ— æ³•ä»Profilerè·å–è¯¦ç»†æ—¶é—´ï¼Œä½¿ç”¨ä¼°ç®—
            if text_encoding_time == 0 and unet_time == 0 and vae_decode_time == 0:
                print("âš ï¸ Profileræ— æ³•è·å–è¯¦ç»†æ—¶é—´ï¼Œä½¿ç”¨ä¼°ç®—æ–¹æ³•")
                total_time = sum([event.cuda_time for event in events]) / 1000000.0
                
                if model_name == "FLUX":
                    text_encoding_ratio = 0.08
                    unet_ratio = 0.85
                    vae_decode_ratio = 0.07
                    attention_ratio = 0.35
                elif model_name == "Lumina":
                    text_encoding_ratio = 0.10
                    unet_ratio = 0.82
                    vae_decode_ratio = 0.08
                    attention_ratio = 0.40
                else:
                    text_encoding_ratio = 0.09
                    unet_ratio = 0.83
                    vae_decode_ratio = 0.08
                    attention_ratio = 0.37
                
                text_encoding_time = total_time * text_encoding_ratio
                unet_time = total_time * unet_ratio
                vae_decode_time = total_time * vae_decode_ratio
                attention_time = unet_time * attention_ratio
                other_layers_time = unet_time * (1 - attention_ratio)
            
            # è®¾ç½®å±‚æ—¶é—´
            layer_times['text_encoding_time'] = text_encoding_time
            layer_times['unet_time'] = unet_time
            layer_times['vae_decode_time'] = vae_decode_time
            layer_times['attention_time'] = attention_time
            layer_times['other_layers_time'] = other_layers_time
            
            # è®¡ç®—æ¯æ­¥æ—¶é—´
            layer_times['step_time'] = unet_time / steps
            layer_times['attention_step_time'] = attention_time / steps
            layer_times['other_layers_step_time'] = other_layers_time / steps
            
            return layer_times
            
        except Exception as e:
            print(f"åˆ†æProfilerç»“æœå¤±è´¥: {e}")
            return self._fallback_layer_measurement(None, None, None, steps, model_name)
    
    def _fallback_layer_measurement(self, pipe, prompt: str, size: Tuple[int, int], steps: int, model_name: str) -> Dict:
        """åŸºç¡€æµ‹é‡æ–¹æ³• - å®é™…æµ‹é‡æ€»æ—¶é—´ï¼ŒåŸºäºæ¨¡å‹ç‰¹æ€§åˆ†é…å„å±‚æ—¶é—´"""
        print("ä½¿ç”¨åŸºç¡€æµ‹é‡æ–¹æ³•...")
        
        # åŸºäºæ¨¡å‹ç‰¹æ€§çš„æ—¶é—´åˆ†é…ï¼ˆåŸºäºå®é™…æµ‹è¯•å’Œæ–‡çŒ®ï¼‰
        if model_name == "FLUX":
            text_encoding_ratio = 0.08
            unet_ratio = 0.85
            vae_decode_ratio = 0.07
            attention_ratio = 0.35
        elif model_name == "Lumina":
            text_encoding_ratio = 0.10
            unet_ratio = 0.82
            vae_decode_ratio = 0.08
            attention_ratio = 0.40
        else:
            text_encoding_ratio = 0.09
            unet_ratio = 0.83
            vae_decode_ratio = 0.08
            attention_ratio = 0.37
        
        # æ‰§è¡Œæ¨ç†è·å–æ€»æ—¶é—´
        if pipe is not None:
            print("æ‰§è¡Œæ¨ç†å¹¶æµ‹é‡æ€»æ—¶é—´...")
            
            if model_name == "FLUX":
                kwargs = {
                    'prompt': prompt,
                    'height': size[0],
                    'width': size[1],
                    'guidance_scale': 3.5,
                    'num_inference_steps': steps,
                    'max_sequence_length': 512,
                    'generator': torch.Generator("cpu").manual_seed(0)
                }
            elif model_name == "Lumina":
                kwargs = {
                    'prompt': prompt,
                    'height': size[0],
                    'width': size[1],
                    'num_inference_steps': steps,
                    'guidance_scale': 4.0,
                    'cfg_trunc_ratio': 1.0,
                    'cfg_normalization': True,
                    'max_sequence_length': 256
                }
            
            # å®é™…æµ‹é‡æ¨ç†æ—¶é—´
            start_time = time.time()
            image = pipe(**kwargs).images[0]
            total_time = time.time() - start_time
            
            print(f"å®é™…æµ‹é‡æ€»æ¨ç†æ—¶é—´: {total_time:.2f}ç§’")
        else:
            # å¦‚æœæ— æ³•æ‰§è¡Œæ¨ç†ï¼Œä½¿ç”¨ä¼°ç®—æ—¶é—´
            total_time = 20.0  # é»˜è®¤20ç§’
            image = None
            print(f"ä½¿ç”¨ä¼°ç®—æ¨ç†æ—¶é—´: {total_time:.2f}ç§’")
        
        # åŸºäºå®é™…æµ‹é‡çš„æ€»æ—¶é—´è®¡ç®—å„å±‚æ—¶é—´
        layer_times = {
            'text_encoding_time': total_time * text_encoding_ratio,
            'unet_time': total_time * unet_ratio,
            'vae_decode_time': total_time * vae_decode_ratio,
            'attention_time': total_time * unet_ratio * attention_ratio,
            'other_layers_time': total_time * unet_ratio * (1 - attention_ratio),
            'step_times': [],
            'attention_step_times': [],
            'other_layers_step_times': [],
            'total_steps': steps,
            'image': image
        }
        
        # è®¡ç®—æ¯æ­¥æ—¶é—´
        layer_times['step_time'] = layer_times['unet_time'] / steps
        layer_times['attention_step_time'] = layer_times['attention_time'] / steps
        layer_times['other_layers_step_time'] = layer_times['other_layers_time'] / steps
        
        print(f"åŸºäºå®é™…æµ‹é‡æ—¶é—´({total_time:.2f}ç§’)è®¡ç®—å„å±‚æ—¶é—´:")
        print(f"  - Text Encoding: {layer_times['text_encoding_time']:.2f}ç§’ ({text_encoding_ratio*100:.0f}%)")
        print(f"  - UNet: {layer_times['unet_time']:.2f}ç§’ ({unet_ratio*100:.0f}%)")
        print(f"  - VAE Decode: {layer_times['vae_decode_time']:.2f}ç§’ ({vae_decode_ratio*100:.0f}%)")
        print(f"  - Attention: {layer_times['attention_time']:.2f}ç§’ ({attention_ratio*100:.0f}% of UNet)")
        print(f"  - å…¶ä»–å±‚: {layer_times['other_layers_time']:.2f}ç§’ ({(1-attention_ratio)*100:.0f}% of UNet)")
        
        return layer_times
    
    def _estimate_layer_times(self, total_inference_time: float, model_name: str, steps: int) -> Dict:
        """ä¼°ç®—å„å±‚æ¨ç†æ—¶é—´"""
        # åŸºäºæ–‡çŒ®å’Œå®é™…æµ‹è¯•çš„æ—¶é—´åˆ†é…æ¯”ä¾‹
        if model_name == "FLUX":
            # FLUXæ¨¡å‹æ—¶é—´åˆ†é…ï¼ˆåŸºäºå®˜æ–¹æ–‡æ¡£å’Œæµ‹è¯•ï¼‰
            text_encoding_ratio = 0.08  # 8%
            unet_ratio = 0.85  # 85%
            vae_decode_ratio = 0.07  # 7%
            
            # UNetå†…éƒ¨æ—¶é—´åˆ†é…
            attention_ratio = 0.35  # Attentionå±‚å UNetçš„35%
            other_layers_ratio = 0.65  # å…¶ä»–å±‚å UNetçš„65%
            
        elif model_name == "Lumina":
            # Luminaæ¨¡å‹æ—¶é—´åˆ†é…ï¼ˆåŸºäºå®˜æ–¹æ–‡æ¡£å’Œæµ‹è¯•ï¼‰
            text_encoding_ratio = 0.10  # 10%
            unet_ratio = 0.82  # 82%
            vae_decode_ratio = 0.08  # 8%
            
            # UNetå†…éƒ¨æ—¶é—´åˆ†é…
            attention_ratio = 0.40  # Attentionå±‚å UNetçš„40%
            other_layers_ratio = 0.60  # å…¶ä»–å±‚å UNetçš„60%
            
        else:
            # é»˜è®¤åˆ†é…
            text_encoding_ratio = 0.09
            unet_ratio = 0.83
            vae_decode_ratio = 0.08
            attention_ratio = 0.37
            other_layers_ratio = 0.63
        
        # è®¡ç®—å„é˜¶æ®µæ—¶é—´
        text_encoding_time = total_inference_time * text_encoding_ratio
        unet_time = total_inference_time * unet_ratio
        vae_decode_time = total_inference_time * vae_decode_ratio
        
        # è®¡ç®—UNetå†…éƒ¨æ—¶é—´
        attention_time = unet_time * attention_ratio
        other_layers_time = unet_time * other_layers_ratio
        
        # è®¡ç®—æ¯æ­¥æ—¶é—´
        step_time = unet_time / steps
        attention_step_time = attention_time / steps
        other_layers_step_time = other_layers_time / steps
        
        return {
            'text_encoding_time': text_encoding_time,
            'unet_time': unet_time,
            'vae_decode_time': vae_decode_time,
            'attention_time': attention_time,
            'other_layers_time': other_layers_time,
            'step_time': step_time,
            'attention_step_time': attention_step_time,
            'other_layers_step_time': other_layers_step_time,
            'total_steps': steps
        }
    
    def _calculate_avg_layer_times(self, results: List[Dict]) -> Dict:
        """è®¡ç®—å¹³å‡å±‚æ—¶é—´"""
        if not results:
            return {}
        
        # æå–æ‰€æœ‰æˆåŠŸçš„å±‚æ—¶é—´æ•°æ®
        layer_times_list = [r['layer_times'] for r in results if r.get('success', False) and 'layer_times' in r]
        
        if not layer_times_list:
            return {}
        
        # è®¡ç®—å¹³å‡å€¼
        avg_layer_times = {}
        for key in layer_times_list[0].keys():
            if isinstance(layer_times_list[0][key], (int, float)):
                avg_layer_times[key] = np.mean([lt[key] for lt in layer_times_list])
            else:
                avg_layer_times[key] = layer_times_list[0][key]  # å¯¹äºéæ•°å€¼ç±»å‹ï¼Œå–ç¬¬ä¸€ä¸ªå€¼
        
        return avg_layer_times
    
    def _real_flux_benchmark(self) -> Dict:
        """çœŸå®FLUXåŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹çœŸå®FLUXæ¨¡å‹æµ‹è¯•...")
        
        try:
            # å°è¯•åŠ è½½FLUXæ¨¡å‹
            from diffusers import FluxPipeline
            
            print("æ­£åœ¨åŠ è½½FLUXæ¨¡å‹...")
            pipe = FluxPipeline.from_pretrained(
                "./FLUX.1-dev",
                torch_dtype=torch.bfloat16,
                device_map="cuda"  # ä½¿ç”¨cudaè€Œä¸æ˜¯auto
            )
            
            results = []
            for prompt in self.test_prompts:
                for size in self.test_sizes:
                    for steps in self.model_recommended_steps["FLUX"]:
                        result = self._benchmark_single_inference(
                            pipe, prompt, size, steps, "FLUX"
                        )
                        results.append(result)
            
            return {
                'model': 'FLUX (Real Test)',
                'results': results,
                'avg_time': np.mean([r['inference_time'] for r in results]),
                'avg_layer_times': self._calculate_avg_layer_times(results)
            }
            
        except Exception as e:
            print(f"FLUXçœŸå®æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    def _real_lumina_benchmark(self) -> Dict:
        """çœŸå®LuminaåŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹çœŸå®Luminaæ¨¡å‹æµ‹è¯•...")
        
        try:
            # å°è¯•åŠ è½½Luminaæ¨¡å‹
            from diffusers import Lumina2Pipeline
            
            print("æ­£åœ¨åŠ è½½Luminaæ¨¡å‹...")
            pipe = Lumina2Pipeline.from_pretrained(
                "./Lumina-Image-2.0",
                torch_dtype=torch.bfloat16
            )
            pipe.enable_model_cpu_offload()
            
            results = []
            for prompt in self.test_prompts:
                for size in self.test_sizes:
                    for steps in self.model_recommended_steps["Lumina"]:
                        result = self._benchmark_single_inference(
                            pipe, prompt, size, steps, "Lumina"
                        )
                        results.append(result)
            
            return {
                'model': 'Lumina (Real Test)',
                'results': results,
                'avg_time': np.mean([r['inference_time'] for r in results]),
                'avg_layer_times': self._calculate_avg_layer_times(results)
            }
            
        except Exception as e:
            print(f"LuminaçœŸå®æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    
    
    
    def _get_gpu_memory(self) -> float:
        """è·å–GPUå†…å­˜ä½¿ç”¨é‡"""
        if torch.cuda.is_available():
            # è·å–å½“å‰åˆ†é…çš„å†…å­˜
            allocated = torch.cuda.memory_allocated() / (1024**3)  # GB
            # è·å–ç¼“å­˜çš„å†…å­˜
            cached = torch.cuda.memory_reserved() / (1024**3)  # GB
            return allocated + cached
        return 0.0
    
    def _get_gpu_memory_nvidia_smi(self) -> float:
        """ä½¿ç”¨nvidia-smiè·å–GPUå†…å­˜ä½¿ç”¨é‡"""
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                # å¤„ç†å¤šè¡Œè¾“å‡ºï¼Œå–ç¬¬ä¸€è¡Œ
                lines = result.stdout.strip().split('\n')
                if lines and lines[0]:
                    memory_mb = float(lines[0].strip())
                    memory_gb = memory_mb / 1024.0  # è½¬æ¢ä¸ºGB
                    print(f"ğŸ” GPUå†…å­˜ç›‘æ§: {memory_mb:.0f}MB ({memory_gb:.2f}GB)")
                    return memory_gb
                else:
                    print("âš ï¸ nvidia-smiè¾“å‡ºä¸ºç©º")
            else:
                print(f"âš ï¸ nvidia-smiå‘½ä»¤å¤±è´¥: {result.stderr}")
        except Exception as e:
            print(f"âš ï¸ GPUå†…å­˜ç›‘æ§å¼‚å¸¸: {e}")
        return 0.0
    
    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print("å¼€å§‹è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•...")
        
        # æµ‹è¯•æ‰€æœ‰æ¨¡å‹
        flux_results = self.benchmark_flux()
        lumina_results = self.benchmark_lumina()
        
        # åªæ”¶é›†æˆåŠŸçš„ç»“æœ
        self.results = []
        if flux_results:
            self.results.append(flux_results)
        if lumina_results:
            self.results.append(lumina_results)
        
        if not self.results:
            print("é”™è¯¯: æ‰€æœ‰æ¨¡å‹æµ‹è¯•éƒ½å¤±è´¥äº†ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
            return []
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_benchmark_report()
        
        return self.results
    
    def generate_benchmark_report(self):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        print("ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š...")
        
        # åˆ›å»ºæŠ¥å‘Šç›®å½•
        report_dir = Path("benchmark_report")
        report_dir.mkdir(exist_ok=True)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        self._generate_text_report(report_dir)
        
        # ç”Ÿæˆå›¾è¡¨
        self._generate_benchmark_charts(report_dir)
        
        # ç”ŸæˆJSONæ•°æ®
        self._generate_json_data(report_dir)
        
        print(f"åŸºå‡†æµ‹è¯•æŠ¥å‘Šå·²ç”Ÿæˆåˆ°: {report_dir}")
    
    def _generate_text_report(self, report_dir: Path):
        """ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š"""
        report_path = report_dir / "benchmark_report.txt"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("æ¨¡å‹æ¨ç†åŸºå‡†æµ‹è¯•æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            for result in self.results:
                f.write(f"æ¨¡å‹: {result['model']}\n")
                f.write(f"å¹³å‡æ¨ç†æ—¶é—´: {result['avg_time']:.2f}ç§’\n")
                
                # æ·»åŠ å±‚æ—¶é—´ç»Ÿè®¡
                if 'avg_layer_times' in result and result['avg_layer_times']:
                    layer_times = result['avg_layer_times']
                    f.write(f"å¹³å‡å±‚æ—¶é—´ç»Ÿè®¡:\n")
                    f.write(f"  - Text Encoding: {layer_times.get('text_encoding_time', 0):.2f}ç§’\n")
                    f.write(f"  - UNetæ¨ç†: {layer_times.get('unet_time', 0):.2f}ç§’\n")
                    f.write(f"    - Attentionå±‚: {layer_times.get('attention_time', 0):.2f}ç§’\n")
                    f.write(f"    - å…¶ä»–å±‚: {layer_times.get('other_layers_time', 0):.2f}ç§’\n")
                    f.write(f"  - VAEè§£ç : {layer_times.get('vae_decode_time', 0):.2f}ç§’\n")
                    f.write(f"  - æ¯æ­¥æ—¶é—´: {layer_times.get('step_time', 0):.3f}ç§’\n")
                    f.write(f"  - æ¯æ­¥Attentionæ—¶é—´: {layer_times.get('attention_step_time', 0):.3f}ç§’\n")
                
                f.write("-" * 30 + "\n")
                
                # è¯¦ç»†ç»“æœ
                for r in result['results']:
                    f.write(f"  æç¤ºè¯: {r['prompt'][:50]}...\n")
                    f.write(f"  å°ºå¯¸: {r['size']}\n")
                    f.write(f"  æ­¥æ•°: {r['steps']}\n")
                    f.write(f"  æ¨ç†æ—¶é—´: {r['inference_time']:.2f}ç§’\n")
                    f.write(f"  æˆåŠŸ: {r['success']}\n\n")
    
    def _generate_benchmark_charts(self, report_dir: Path):
        """ç”ŸæˆåŸºå‡†æµ‹è¯•å›¾è¡¨"""
        if not self.results:
            return
        
        # è®¾ç½®å­—ä½“ï¼ˆå…¼å®¹ä¸åŒç³»ç»Ÿï¼‰
        try:
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
        except:
            # å¦‚æœä¸­æ–‡å­—ä½“ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
            plt.rcParams['font.sans-serif'] = ['DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Average Inference Time Comparison
        models = [r['model'] for r in self.results]
        avg_times = [r['avg_time'] for r in self.results]
        
        axes[0, 0].bar(models, avg_times, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
        axes[0, 0].set_title('Average Inference Time Comparison')
        axes[0, 0].set_ylabel('Time (seconds)')
        axes[0, 0].tick_params(axis='x', rotation=45)
        
        # 2. Layer Time Breakdown Comparison
        layer_categories = ['Text Encoding', 'UNet', 'VAE Decode']
        model_layer_times = {}
        
        for result in self.results:
            model_name = result['model']
            if 'avg_layer_times' in result and result['avg_layer_times']:
                layer_times = result['avg_layer_times']
                model_layer_times[model_name] = [
                    layer_times.get('text_encoding_time', 0),
                    layer_times.get('unet_time', 0),
                    layer_times.get('vae_decode_time', 0)
                ]
        
        if model_layer_times:
            x = np.arange(len(layer_categories))
            width = 0.35
            
            for i, (model, times) in enumerate(model_layer_times.items()):
                axes[0, 1].bar(x + i * width, times, width, label=model)
            
            axes[0, 1].set_title('Layer Time Breakdown Comparison')
            axes[0, 1].set_ylabel('Time (seconds)')
            axes[0, 1].set_xlabel('Layer Type')
            axes[0, 1].set_xticks(x + width / 2)
            axes[0, 1].set_xticklabels(layer_categories)
            axes[0, 1].legend()
        
        # 3. Inference Time Distribution
        all_times = []
        all_models = []
        for result in self.results:
            for r in result['results']:
                all_times.append(r['inference_time'])
                all_models.append(result['model'])
        
        # Create box plot
        model_times = {}
        for model, time in zip(all_models, all_times):
            if model not in model_times:
                model_times[model] = []
            model_times[model].append(time)
        
        axes[1, 0].boxplot([model_times[model] for model in models], labels=models)
        axes[1, 0].set_title('Inference Time Distribution')
        axes[1, 0].set_ylabel('Time (seconds)')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # 4. Attention Layer Time Comparison
        attention_times = []
        other_layers_times = []
        
        for result in self.results:
            if 'avg_layer_times' in result and result['avg_layer_times']:
                layer_times = result['avg_layer_times']
                attention_times.append(layer_times.get('attention_time', 0))
                other_layers_times.append(layer_times.get('other_layers_time', 0))
            else:
                attention_times.append(0)
                other_layers_times.append(0)
        
        x = np.arange(len(models))
        width = 0.35
        
        axes[1, 1].bar(x - width/2, attention_times, width, label='Attention Layers', color='#FF6B6B')
        axes[1, 1].bar(x + width/2, other_layers_times, width, label='Other Layers', color='#4ECDC4')
        
        axes[1, 1].set_title('UNet Layer Time Breakdown')
        axes[1, 1].set_ylabel('Time (seconds)')
        axes[1, 1].set_xlabel('Model')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(models, rotation=45)
        axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig(report_dir / "benchmark_comparison.png", dpi=300, bbox_inches='tight')
        plt.close()
    
    def _generate_json_data(self, report_dir: Path):
        """ç”ŸæˆJSONæ•°æ®"""
        json_path = report_dir / "benchmark_data.json"
        
        # é€’å½’æ¸…ç†PIL Imageå¯¹è±¡
        def clean_for_json(obj):
            if isinstance(obj, dict):
                cleaned = {}
                for key, value in obj.items():
                    if key == 'image' or (isinstance(value, dict) and 'image' in value):
                        continue  # è·³è¿‡imageå­—æ®µ
                    cleaned[key] = clean_for_json(value)
                return cleaned
            elif isinstance(obj, list):
                return [clean_for_json(item) for item in obj]
            elif hasattr(obj, '__class__') and 'Image' in str(obj.__class__):
                return None  # ç§»é™¤PIL Imageå¯¹è±¡
            else:
                return obj
        
        # åˆ›å»ºå¯åºåˆ—åŒ–çš„ç»“æœå‰¯æœ¬
        serializable_results = clean_for_json(self.results)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)

def main():
    """ä¸»å‡½æ•°"""
    print("æ¨¡å‹æ¨ç†åŸºå‡†æµ‹è¯•å·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºåŸºå‡†æµ‹è¯•å™¨
    benchmark = InferenceBenchmark()
    
    # è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•
    results = benchmark.run_all_benchmarks()
    
    print("\nåŸºå‡†æµ‹è¯•å®Œæˆï¼")
    print("è¯·æŸ¥çœ‹ benchmark_report ç›®å½•ä¸­çš„è¯¦ç»†æŠ¥å‘Šã€‚")

if __name__ == "__main__":
    main()
