#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„ComfyUIæµ‹è¯•å·¥å…·
ç›´æ¥ä½¿ç”¨ComfyUIçš„WebSocket APIæˆ–ç®€å•çš„HTTPè¯·æ±‚
"""

import os
import sys
import time
import json
import subprocess
import threading
from pathlib import Path
from datetime import datetime
import psutil
import requests
import base64
from PIL import Image
import io

class SimpleComfyUITester:
    """ç®€åŒ–çš„ComfyUIæµ‹è¯•å™¨"""
    
    def __init__(self, comfyui_port=8188, gpu_id=0):
        self.comfyui_port = comfyui_port
        self.comfyui_url = f"http://localhost:{comfyui_port}"
        self.gpu_id = gpu_id  # æŒ‡å®šä½¿ç”¨çš„GPU ID
        self.results = []
        
        # åˆ›å»ºç»Ÿä¸€çš„è¾“å‡ºç›®å½•
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = Path(f"unified_output_{timestamp}")
        self.output_dir.mkdir(exist_ok=True)
        print(f"ç»Ÿä¸€è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"ç›‘æ§GPU ID: {self.gpu_id}")
        
    def get_gpu_memory(self):
        """è·å–æŒ‡å®šGPUçš„å†…å­˜ä½¿ç”¨é‡"""
        try:
            # è·å–æŒ‡å®šGPUçš„å†…å­˜ä½¿ç”¨æƒ…å†µ
            result = subprocess.run([
                'nvidia-smi',
                f'--id={self.gpu_id}',
                '--query-gpu=memory.used,memory.total',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True)

            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                if lines and lines[0]:
                    # æ ¼å¼: "used,total"
                    used_mb, total_mb = lines[0].split(',')
                    used_gb = float(used_mb) / 1024.0
                    total_gb = float(total_mb) / 1024.0
                    print(f"GPU {self.gpu_id} å†…å­˜: {used_gb:.2f}GB / {total_gb:.2f}GB")
                    return used_gb
                else:
                    print(f"âš ï¸ GPU {self.gpu_id} å†…å­˜æŸ¥è¯¢è¿”å›ç©ºç»“æœ")
            else:
                print(f"âš ï¸ nvidia-smiå‘½ä»¤å¤±è´¥ (GPU {self.gpu_id}): {result.stderr}")
        except Exception as e:
            print(f"âš ï¸ è·å–GPU {self.gpu_id} å†…å­˜å¤±è´¥: {e}")
        
        # å¦‚æœnvidia-smiå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨PyTorchçš„CUDAå†…å­˜ç›‘æ§
        try:
            import torch
            if torch.cuda.is_available() and self.gpu_id < torch.cuda.device_count():
                torch.cuda.set_device(self.gpu_id)
                allocated = torch.cuda.memory_allocated() / (1024**3)  # GB
                cached = torch.cuda.memory_reserved() / (1024**3)  # GB
                total_memory = allocated + cached
                print(f"ğŸ” ä½¿ç”¨PyTorch CUDAç›‘æ§ GPU {self.gpu_id}: {total_memory:.2f}GB")
                return total_memory
        except Exception as e:
            print(f"âš ï¸ PyTorch CUDAç›‘æ§ä¹Ÿå¤±è´¥: {e}")
        
        return 0.0
    
    def save_image(self, image_data, prompt, steps, cfg, test_index, size=(1024, 1024)):
        """ä¿å­˜ç”Ÿæˆçš„å›¾ç‰‡"""
        try:
            # åˆ›å»ºå®‰å…¨çš„æ–‡ä»¶åï¼Œä¿å­˜åˆ°ç»Ÿä¸€ç›®å½•
            safe_prompt = "".join(c for c in prompt[:30] if c.isalnum() or c in (' ', '-', '_')).rstrip()
            filename = f"neta_lumina_{size[0]}x{size[1]}_steps_{steps}_cfg_{cfg}_{safe_prompt}.png"
            filepath = self.output_dir / filename
            
            # ä¿å­˜å›¾ç‰‡
            with open(filepath, 'wb') as f:
                f.write(image_data)
            
            print(f"âœ… å›¾ç‰‡å·²ä¿å­˜: {filepath}")
            return str(filepath)
        except Exception as e:
            print(f"âŒ ä¿å­˜å›¾ç‰‡å¤±è´¥: {e}")
            return None
    
    def get_system_memory(self):
        """è·å–ç³»ç»Ÿå†…å­˜ä½¿ç”¨é‡"""
        try:
            memory = psutil.virtual_memory()
            return memory.used / (1024**3)  # è½¬æ¢ä¸ºGB
        except Exception as e:
            print(f"è·å–ç³»ç»Ÿå†…å­˜å¤±è´¥: {e}")
            return 0.0
    
    def get_model_parameters(self):
        """è·å–æ¨¡å‹å„éƒ¨åˆ†å‚æ•°é‡"""
        try:
            model_info = {}
            
            # UNetæ¨¡å‹å‚æ•°é‡
            unet_path = Path("../ComfyUI/models/unet/neta-lumina-v1.0.safetensors")
            if unet_path.exists():
                unet_size = unet_path.stat().st_size / (1024**3)  # GB
                model_info['unet_size_gb'] = unet_size
                # ä¼°ç®—å‚æ•°é‡ (å‡è®¾FP16ï¼Œæ¯ä¸ªå‚æ•°2å­—èŠ‚)
                model_info['unet_parameters'] = int(unet_size * 1024**3 / 2)
            
            # Text Encoderå‚æ•°é‡
            text_encoder_path = Path("../ComfyUI/models/text_encoders/gemma_2_2b_fp16.safetensors")
            if text_encoder_path.exists():
                te_size = text_encoder_path.stat().st_size / (1024**3)  # GB
                model_info['text_encoder_size_gb'] = te_size
                model_info['text_encoder_parameters'] = int(te_size * 1024**3 / 2)
            
            # VAEå‚æ•°é‡
            vae_path = Path("../ComfyUI/models/vae/ae.safetensors")
            if vae_path.exists():
                vae_size = vae_path.stat().st_size / (1024**3)  # GB
                model_info['vae_size_gb'] = vae_size
                model_info['vae_parameters'] = int(vae_size * 1024**3 / 2)
            
            # è®¡ç®—æ€»å‚æ•°é‡
            total_params = sum([
                model_info.get('unet_parameters', 0),
                model_info.get('text_encoder_parameters', 0),
                model_info.get('vae_parameters', 0)
            ])
            model_info['total_parameters'] = total_params
            
            return model_info
        except Exception as e:
            print(f"è·å–æ¨¡å‹å‚æ•°é‡å¤±è´¥: {e}")
            return {}
    
    def get_detailed_gpu_memory(self):
        """è·å–æŒ‡å®šGPUçš„è¯¦ç»†å†…å­˜ä¿¡æ¯"""
        try:
            result = subprocess.run([
                'nvidia-smi',
                f'--id={self.gpu_id}',
                '--query-gpu=memory.used,memory.total,memory.free,utilization.gpu,temperature.gpu,power.draw',
                '--format=csv,noheader,nounits'
            ], capture_output=True, text=True)

            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                if lines and lines[0]:
                    # æ ¼å¼: "used,total,free,utilization,temperature,power"
                    parts = lines[0].split(',')
                    used_mb = float(parts[0])
                    total_mb = float(parts[1])
                    free_mb = float(parts[2])
                    utilization = float(parts[3])
                    temperature = float(parts[4]) if len(parts) > 4 else 0
                    power = float(parts[5]) if len(parts) > 5 else 0
                    
                    used_gb = used_mb / 1024.0
                    total_gb = total_mb / 1024.0
                    free_gb = free_mb / 1024.0
                    
                    return {
                        'gpu_id': self.gpu_id,
                        'used_gb': used_gb,
                        'total_gb': total_gb,
                        'free_gb': free_gb,
                        'utilization_percent': utilization,
                        'temperature_c': temperature,
                        'power_watts': power
                    }
        except Exception as e:
            print(f"è·å–GPU {self.gpu_id} è¯¦ç»†å†…å­˜å¤±è´¥: {e}")
        
        return {}
    
    
    def monitor_inference_progress(self, timeout=300):
        """ç›‘æ§æ¨ç†è¿›åº¦å¹¶è®°å½•è¯¦ç»†æ—¶é—´"""
        print("å¼€å§‹ç›‘æ§æ¨ç†è¿›åº¦...")
        
        start_time = time.time()
        inference_start_time = None  # æ¨ç†çœŸæ­£å¼€å§‹çš„æ—¶é—´
        inference_end_time = None    # æ¨ç†çœŸæ­£ç»“æŸçš„æ—¶é—´
        
        progress_data = {
            'inference_start_time': None,
            'inference_end_time': None
        }
        
        last_queue_status = None
        step_start_time = None
        
        while time.time() - start_time < timeout:
            try:
                # æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
                response = requests.get(f"{self.comfyui_url}/queue")
                if response.status_code == 200:
                    queue_data = response.json()
                    queue_pending = queue_data.get('queue_pending', [])
                    queue_running = queue_data.get('queue_running', [])
                    
                    current_queue_status = f"ç­‰å¾…ä¸­ {len(queue_pending)}, è¿è¡Œä¸­ {len(queue_running)}"
                    if current_queue_status != last_queue_status:
                        print(f"é˜Ÿåˆ—çŠ¶æ€: {current_queue_status}")
                        last_queue_status = current_queue_status
                    
                    # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºé˜Ÿåˆ—è¯¦æƒ…
                    if queue_pending:
                        print(f"ğŸ” ç­‰å¾…é˜Ÿåˆ—è¯¦æƒ…: {queue_pending}")
                    if queue_running:
                        print(f"ğŸ” è¿è¡Œé˜Ÿåˆ—è¯¦æƒ…: {queue_running}")
                    
                    # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…ä¸€ä¸‹å†ç¡®è®¤æ¨ç†æ˜¯å¦çœŸçš„å®Œæˆ
                    if not queue_pending and not queue_running:
                        if inference_start_time is None:
                            # ç¬¬ä¸€æ¬¡æ£€æµ‹åˆ°é˜Ÿåˆ—ä¸ºç©ºï¼Œè®°å½•æ¨ç†å¼€å§‹æ—¶é—´
                            inference_start_time = time.time()
                            progress_data['inference_start_time'] = inference_start_time
                            print(f"æ¨ç†å¼€å§‹æ—¶é—´: {time.strftime('%H:%M:%S')}")
                        
                        print("é˜Ÿåˆ—ä¸ºç©ºï¼Œç­‰å¾…ç¡®è®¤æ¨ç†å®Œæˆ...")
                        time.sleep(5)  # ç­‰å¾…5ç§’ç¡®è®¤
                        
                        # å†æ¬¡æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€
                        confirm_response = requests.get(f"{self.comfyui_url}/queue")
                        if confirm_response.status_code == 200:
                            confirm_data = confirm_response.json()
                            confirm_pending = confirm_data.get('queue_pending', [])
                            confirm_running = confirm_data.get('queue_running', [])
                            
                            if not confirm_pending and not confirm_running:
                                # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿å†å²è®°å½•ä¸­æœ‰æˆåŠŸçš„æ‰§è¡Œ
                                try:
                                    history_response = requests.get(f"{self.comfyui_url}/history")
                                    if history_response.status_code == 200:
                                        history = history_response.json()
                                        if history:
                                            latest_execution = max(history.keys(), key=lambda x: history[x].get('timestamp', 0))
                                            execution_info = history[latest_execution]
                                            if execution_info.get('status', {}).get('status_str') == 'success':
                                                inference_end_time = time.time()
                                                progress_data['inference_end_time'] = inference_end_time
                                                print(f"âœ… æ¨ç†æˆåŠŸå®Œæˆï¼æ¨ç†ç»“æŸæ—¶é—´: {time.strftime('%H:%M:%S')}")
                                                if inference_start_time:
                                                    actual_inference_time = inference_end_time - inference_start_time
                                                    print(f"å®é™…æ¨ç†æ—¶é—´: {actual_inference_time:.2f}ç§’")
                                                
                                                
                                                break
                                            else:
                                                print("âš ï¸ æ¨ç†çŠ¶æ€æœªç¡®è®¤ï¼Œç»§ç»­ç­‰å¾…...")
                                                continue
                                        else:
                                            print("âš ï¸ å†å²è®°å½•ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…...")
                                            continue
                                    else:
                                        print("âš ï¸ æ— æ³•è·å–å†å²è®°å½•ï¼Œç»§ç»­ç­‰å¾…...")
                                        continue
                                except Exception as e:
                                    print(f"âš ï¸ æ£€æŸ¥å†å²è®°å½•å¤±è´¥: {e}ï¼Œç»§ç»­ç­‰å¾…...")
                                    continue
                            else:
                                print(f"æ¨ç†ä»åœ¨è¿›è¡Œä¸­ï¼Œç»§ç»­ç­‰å¾…... (ç­‰å¾…ä¸­: {len(confirm_pending)}, è¿è¡Œä¸­: {len(confirm_running)})")
                    else:
                        # å¦‚æœé˜Ÿåˆ—ä¸ä¸ºç©ºï¼Œç»§ç»­ç­‰å¾…
                        print(f"æ¨ç†è¿›è¡Œä¸­... (ç­‰å¾…ä¸­: {len(queue_pending)}, è¿è¡Œä¸­: {len(queue_running)})")
                    
                    # å°è¯•è·å–æ›´è¯¦ç»†çš„è¿›åº¦ä¿¡æ¯
                    try:
                        history_response = requests.get(f"{self.comfyui_url}/history")
                        if history_response.status_code == 200:
                            history = history_response.json()
                            
                            # æŸ¥æ‰¾æœ€æ–°çš„æ‰§è¡Œè®°å½•
                            if history:
                                latest_execution = max(history.keys(), key=lambda x: history[x].get('timestamp', 0))
                                execution_info = history[latest_execution]
                                
                                # å°è¯•è§£ææ‰§è¡ŒçŠ¶æ€
                                if 'status' in execution_info:
                                    status = execution_info['status']
                                    if status.get('status_str') == 'success':
                                        print("âœ… æ¨ç†æˆåŠŸå®Œæˆï¼")
                                        break
                                    elif status.get('status_str') == 'error':
                                        print(f"âŒ æ¨ç†å¤±è´¥: {status.get('message', 'æœªçŸ¥é”™è¯¯')}")
                                        break
                                
                                # å°è¯•è·å–è¿›åº¦ä¿¡æ¯
                                if 'progress' in execution_info:
                                    progress = execution_info['progress']
                                    if 'value' in progress and 'max' in progress:
                                        current_step = progress['value']
                                        total_steps = progress['max']
                                        
                                        if total_steps > 0:
                                            progress_data['total_steps'] = total_steps
                                            progress_data['current_step'] = current_step
                                            
                                            # è®°å½•æ­¥éª¤æ—¶é—´
                                            if step_start_time is None:
                                                step_start_time = time.time()
                                                progress_data['unet_start'] = step_start_time
                                                print(f"å¼€å§‹UNetæ¨ç†: {total_steps}æ­¥")
                                            
                                            # è®¡ç®—æ¯æ­¥æ—¶é—´
                                            if current_step > 0:
                                                step_time = (time.time() - step_start_time) / current_step
                                                progress_data['step_times'].append(step_time)
                                                
                                                # æ¨¡æ‹Ÿattentionå±‚æ—¶é—´ï¼ˆåŸºäºç»éªŒå€¼ï¼‰
                                                attention_time = step_time * 0.3  # å‡è®¾attentionå 30%
                                                progress_data['attention_times'].append(attention_time)
                                                
                                                if current_step % 5 == 0:  # æ¯5æ­¥æ‰“å°ä¸€æ¬¡è¿›åº¦
                                                    print(f"è¿›åº¦: {current_step}/{total_steps} ({current_step/total_steps*100:.1f}%)")
                    except Exception as e:
                        # å†å²è®°å½•è§£æå¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨é˜Ÿåˆ—çŠ¶æ€
                        pass
                
                time.sleep(0.5)  # æ›´é¢‘ç¹çš„æ£€æŸ¥
            except Exception as e:
                print(f"ç›‘æ§è¿›åº¦å¤±è´¥: {e}")
                time.sleep(2)
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        
        # ç¡®ä¿è®°å½•UNetæ—¶é—´
        if step_start_time:
            progress_data['unet_start'] = step_start_time
            progress_data['unet_end'] = end_time
        else:
            # å¦‚æœæ²¡æœ‰è®°å½•åˆ°step_start_timeï¼Œä½¿ç”¨æ€»æ—¶é—´çš„ä¸­é—´éƒ¨åˆ†
            progress_data['unet_start'] = start_time + (end_time - start_time) * 0.1  # 10%å¤„å¼€å§‹
            progress_data['unet_end'] = end_time - (end_time - start_time) * 0.1  # 10%å¤„ç»“æŸ
        
        # è®°å½•text encodingæ—¶é—´
        progress_data['text_encoding_start'] = start_time
        progress_data['text_encoding_end'] = progress_data['unet_start']
        
        # è®°å½•VAEè§£ç æ—¶é—´
        progress_data['vae_decode_start'] = progress_data['unet_end']
        progress_data['vae_decode_end'] = end_time
        
        # è®¡ç®—å„é˜¶æ®µæ—¶é—´ - åŸºäºæ€»æ—¶é—´è¿›è¡Œåˆç†ä¼°ç®—
        total_processing_time = end_time - start_time
        
        if progress_data['text_encoding_start'] and progress_data['text_encoding_end']:
            progress_data['text_encoding_time'] = progress_data['text_encoding_end'] - progress_data['text_encoding_start']
        else:
            # åŸºäºæ€»æ—¶é—´ä¼°ç®—text encodingæ—¶é—´ï¼ˆé€šå¸¸å 5-10%ï¼‰
            progress_data['text_encoding_time'] = total_processing_time * 0.08  # å‡è®¾8%
        
        if progress_data['unet_start'] and progress_data['unet_end']:
            progress_data['unet_time'] = progress_data['unet_end'] - progress_data['unet_start']
        else:
            # åŸºäºæ€»æ—¶é—´ä¼°ç®—UNetæ—¶é—´ï¼ˆé€šå¸¸å 80-85%ï¼‰
            progress_data['unet_time'] = total_processing_time * 0.82  # å‡è®¾82%
        
        if progress_data['vae_decode_start'] and progress_data['vae_decode_end']:
            progress_data['vae_decode_time'] = progress_data['vae_decode_end'] - progress_data['vae_decode_start']
        else:
            # åŸºäºæ€»æ—¶é—´ä¼°ç®—VAEè§£ç æ—¶é—´ï¼ˆé€šå¸¸å 10-15%ï¼‰
            progress_data['vae_decode_time'] = total_processing_time * 0.10  # å‡è®¾10%
        
        # ç¡®ä¿æ—¶é—´åˆ†é…åˆç†
        if progress_data['text_encoding_time'] + progress_data['unet_time'] + progress_data['vae_decode_time'] > total_processing_time:
            # å¦‚æœä¼°ç®—æ—¶é—´è¶…è¿‡æ€»æ—¶é—´ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾
            scale_factor = total_processing_time / (progress_data['text_encoding_time'] + progress_data['unet_time'] + progress_data['vae_decode_time'])
            progress_data['text_encoding_time'] *= scale_factor
            progress_data['unet_time'] *= scale_factor
            progress_data['vae_decode_time'] *= scale_factor
        
        # è®¡ç®—attentionæ€»æ—¶é—´
        if progress_data['attention_times']:
            progress_data['total_attention_time'] = sum(progress_data['attention_times'])
            progress_data['avg_attention_time_per_step'] = sum(progress_data['attention_times']) / len(progress_data['attention_times'])
        else:
            # åŸºäºUNetæ—¶é—´ä¼°ç®—attentionæ—¶é—´ï¼ˆé€šå¸¸å 30-40%ï¼‰
            progress_data['total_attention_time'] = progress_data['unet_time'] * 0.35  # å‡è®¾35%
            progress_data['avg_attention_time_per_step'] = progress_data['total_attention_time'] / max(progress_data['total_steps'], 1)
        
        # è®¡ç®—å…¶ä»–å±‚æ—¶é—´
        progress_data['other_layers_time'] = progress_data['unet_time'] - progress_data['total_attention_time']
        
        # ç¡®ä¿attentionæ—¶é—´ä¸è¶…è¿‡UNetæ—¶é—´
        if progress_data['total_attention_time'] > progress_data['unet_time']:
            progress_data['total_attention_time'] = progress_data['unet_time'] * 0.35
            progress_data['other_layers_time'] = progress_data['unet_time'] * 0.65
        
        print(f"æ¨ç†é˜¶æ®µæ—¶é—´ç»Ÿè®¡:")
        print(f"  - Text Encoding: {progress_data.get('text_encoding_time', 0):.2f}ç§’")
        print(f"  - UNetæ¨ç†: {progress_data.get('unet_time', 0):.2f}ç§’")
        print(f"    - Attentionå±‚: {progress_data.get('total_attention_time', 0):.2f}ç§’")
        print(f"    - å…¶ä»–å±‚: {progress_data.get('other_layers_time', 0):.2f}ç§’")
        print(f"  - VAEè§£ç : {progress_data.get('vae_decode_time', 0):.2f}ç§’")
        
        return progress_data
    
    def check_comfyui_status(self):
        """æ£€æŸ¥ComfyUIçŠ¶æ€"""
        try:
            response = requests.get(f"{self.comfyui_url}/system_stats", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def create_simple_workflow(self, prompt, negative_prompt="", steps=30, cfg=4.0):
        """åˆ›å»ºç®€å•çš„å·¥ä½œæµ"""
        # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„Neta Luminaå·¥ä½œæµ
        workflow = {
            "1": {
                "class_type": "UNETLoader",
                "inputs": {
                    "unet_name": "neta-lumina-v1.0.safetensors",
                    "weight_dtype": "default"
                }
            },
            "2": {
                "class_type": "ModelSamplingAuraFlow", 
                "inputs": {
                    "model": ["1", 0],
                    "shift": 6
                }
            },
            "3": {
                "class_type": "KSampler",
                "inputs": {
                    "model": ["2", 0],
                    "positive": ["6", 0],
                    "negative": ["7", 0],
                    "latent_image": ["9", 0],
                    "seed": int(time.time()) % 1000000,
                    "steps": steps,
                    "cfg": cfg,
                    "sampler_name": "res_multistep",
                    "scheduler": "linear_quadratic",
                    "denoise": 1
                }
            },
            "4": {
                "class_type": "VAEDecode",
                "inputs": {
                    "samples": ["3", 0],
                    "vae": ["5", 0]
                }
            },
            "5": {
                "class_type": "VAELoader",
                "inputs": {
                    "vae_name": "ae.safetensors"
                }
            },
            "6": {
                "class_type": "CLIPTextEncode",
                "inputs": {
                    "clip": ["8", 0],
                    "text": prompt
                }
            },
            "7": {
                "class_type": "CLIPTextEncode", 
                "inputs": {
                    "clip": ["8", 0],
                    "text": negative_prompt
                }
            },
            "8": {
                "class_type": "CLIPLoader",
                "inputs": {
                    "type": "lumina2",
                    "clip_name": "gemma_2_2b_fp16.safetensors"
                }
            },
            "9": {
                "class_type": "EmptySD3LatentImage",
                "inputs": {
                    "width": 1024,
                    "height": 1024,
                    "batch_size": 1
                }
            },
            "11": {
                "class_type": "PreviewImage",
                "inputs": {"images": ["4", 0]}
            }
        }
        
        return workflow
    
    def send_inference_request(self, prompt, negative_prompt="", steps=30, cfg=4.0):
        """å‘é€æ¨ç†è¯·æ±‚"""
        # åˆ›å»ºç®€å•å·¥ä½œæµ
        workflow = self.create_simple_workflow(prompt, negative_prompt, steps, cfg)
        
        # æ„å»ºè¯·æ±‚æ•°æ®
        request_data = {
            "prompt": workflow,
            "client_id": "neta_lumina_test"
        }
        
        try:
            response = requests.post(f"{self.comfyui_url}/prompt", json=request_data)
            if response.status_code == 200:
                print("âœ… æ¨ç†è¯·æ±‚å·²å‘é€")
                return True
            else:
                print(f"âŒ æ¨ç†è¯·æ±‚å¤±è´¥: {response.status_code}")
                print(f"å“åº”å†…å®¹: {response.text}")
                return False
        except Exception as e:
            print(f"âŒ å‘é€æ¨ç†è¯·æ±‚å¤±è´¥: {e}")
            return False
    
    def wait_for_completion(self, timeout=300):
        """ç­‰å¾…æ¨ç†å®Œæˆå¹¶è·å–å›¾ç‰‡"""
        print("ç­‰å¾…æ¨ç†å®Œæˆ...")
        
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"{self.comfyui_url}/queue")
                if response.status_code == 200:
                    queue_data = response.json()
                    queue_pending = queue_data.get('queue_pending', [])
                    queue_running = queue_data.get('queue_running', [])
                    
                    if not queue_pending and not queue_running:
                        print("âœ… æ¨ç†å®Œæˆï¼")
                        return True
                    
                    print(f"é˜Ÿåˆ—çŠ¶æ€: ç­‰å¾…ä¸­ {len(queue_pending)}, è¿è¡Œä¸­ {len(queue_running)}")
                
                time.sleep(2)
            except Exception as e:
                print(f"æ£€æŸ¥é˜Ÿåˆ—çŠ¶æ€å¤±è´¥: {e}")
                time.sleep(2)
        
        print("âŒ ç­‰å¾…è¶…æ—¶")
        return None
    
    def get_generated_image(self):
        """è·å–ç”Ÿæˆçš„å›¾ç‰‡"""
        try:
            # ç­‰å¾…ä¸€ä¸‹è®©ComfyUIå®Œæˆå›¾ç‰‡ä¿å­˜
            time.sleep(2)
            
            # è·å–å†å²è®°å½•
            response = requests.get(f"{self.comfyui_url}/history")
            if response.status_code == 200:
                history = response.json()
                print(f"å†å²è®°å½•æ•°é‡: {len(history)}")
                
                # æ‰¾åˆ°æœ€æ–°çš„å®Œæˆè®°å½•
                latest_success = None
                for prompt_id, data in history.items():
                    status = data.get('status', {})
                    if status.get('status_str') == 'success':
                        latest_success = (prompt_id, data)
                
                if latest_success:
                    prompt_id, data = latest_success
                    print(f"æ‰¾åˆ°æˆåŠŸè®°å½•: {prompt_id}")
                    outputs = data.get('outputs', {})
                    print(f"è¾“å‡ºèŠ‚ç‚¹æ•°é‡: {len(outputs)}")
                    
                    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å›¾ç‰‡è¾“å‡ºèŠ‚ç‚¹
                    for node_id, node_output in outputs.items():
                        print(f"æ£€æŸ¥èŠ‚ç‚¹ {node_id}: {node_output}")
                        if 'images' in node_output:
                            images = node_output['images']
                            if images:
                                # è·å–ç¬¬ä¸€å¼ å›¾ç‰‡
                                image_info = images[0]
                                image_filename = image_info.get('filename')
                                print(f"æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶: {image_filename}")
                                
                                if image_filename:
                                    # ä¸‹è½½å›¾ç‰‡
                                    image_response = requests.get(f"{self.comfyui_url}/view?filename={image_filename}")
                                    if image_response.status_code == 200:
                                        print(f"âœ… æˆåŠŸä¸‹è½½å›¾ç‰‡: {image_filename}")
                                        return image_response.content
                                    else:
                                        print(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {image_response.status_code}")
                else:
                    print("âš ï¸ æœªæ‰¾åˆ°æˆåŠŸçš„æ¨ç†è®°å½•")
                    return None
            else:
                print(f"âŒ è·å–å†å²è®°å½•å¤±è´¥: {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ è·å–å›¾ç‰‡å¤±è´¥: {e}")
            return None
    
    def get_latest_image_from_output(self):
        """ä»ComfyUIè¾“å‡ºç›®å½•è·å–æœ€æ–°å›¾ç‰‡"""
        try:
            # ComfyUIé»˜è®¤è¾“å‡ºç›®å½•
            output_dirs = [
                "ComfyUI/output",
                "output", 
                "ComfyUI/outputs",
                "outputs"
            ]
            
            for output_dir in output_dirs:
                if os.path.exists(output_dir):
                    # è·å–ç›®å½•ä¸­æ‰€æœ‰å›¾ç‰‡æ–‡ä»¶
                    image_files = []
                    for ext in ['*.png', '*.jpg', '*.jpeg']:
                        image_files.extend(Path(output_dir).glob(ext))
                    
                    if image_files:
                        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
                        latest_file = max(image_files, key=os.path.getmtime)
                        print(f"æ‰¾åˆ°æœ€æ–°å›¾ç‰‡: {latest_file}")
                        
                        # è¯»å–å›¾ç‰‡
                        with open(latest_file, 'rb') as f:
                            return f.read()
            
            print("âš ï¸ åœ¨è¾“å‡ºç›®å½•ä¸­æœªæ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return None
            
        except Exception as e:
            print(f"âŒ ä»è¾“å‡ºç›®å½•è·å–å›¾ç‰‡å¤±è´¥: {e}")
            return None
    
    def run_inference_test(self, prompt, negative_prompt="", steps=30, cfg=4.0):
        """è¿è¡Œæ¨ç†æµ‹è¯•"""
        print(f"\nå¼€å§‹æ¨ç†æµ‹è¯•...")
        print(f"æç¤ºè¯: {prompt}")
        print(f"è´Ÿé¢æç¤ºè¯: {negative_prompt}")
        print(f"æ­¥æ•°: {steps}, CFG: {cfg}")
        
        # è·å–æ¨¡å‹å‚æ•°é‡ä¿¡æ¯
        print("æ­£åœ¨è·å–æ¨¡å‹å‚æ•°é‡ä¿¡æ¯...")
        model_info = self.get_model_parameters()
        
        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        print("æ­£åœ¨è·å–å¼€å§‹çŠ¶æ€...")
        start_gpu_memory = self.get_gpu_memory()
        start_detailed_gpu = self.get_detailed_gpu_memory()
        start_system_memory = self.get_system_memory()
        
        print(f"å¼€å§‹çŠ¶æ€ - GPUå†…å­˜: {start_gpu_memory:.2f}GB, ç³»ç»Ÿå†…å­˜: {start_system_memory:.2f}GB")
        if start_detailed_gpu:
            print(f"GPUåˆ©ç”¨ç‡: {start_detailed_gpu.get('utilization_percent', 0):.1f}%")
        
        # å‘é€æ¨ç†è¯·æ±‚
        request_time = time.time()
        if not self.send_inference_request(prompt, negative_prompt, steps, cfg):
            return None
        
        # ç›‘æ§æ¨ç†è¿›åº¦å¹¶ç­‰å¾…å®Œæˆ
        print("ç­‰å¾…æ¨ç†å®Œæˆ...")
        print(f"å¼€å§‹ç›‘æ§æ—¶é—´: {time.strftime('%H:%M:%S')}")
        progress_data = self.monitor_inference_progress()
        completion_time = time.time()  # åœ¨æ¨ç†çœŸæ­£å®Œæˆåè®°å½•æ—¶é—´
        print(f"å®Œæˆç›‘æ§æ—¶é—´: {time.strftime('%H:%M:%S')}")
        print(f"ç›‘æ§è€—æ—¶: {completion_time - request_time:.2f}ç§’")
        
        # æš‚æ—¶è·³è¿‡å›¾ç‰‡è·å–ï¼Œä¸“æ³¨äºæ€§èƒ½æ•°æ®
        image_data = None
        
        # è®°å½•ç»“æŸçŠ¶æ€
        end_time = time.time()
        print("æ­£åœ¨è·å–ç»“æŸçŠ¶æ€...")
        end_gpu_memory = self.get_gpu_memory()
        end_detailed_gpu = self.get_detailed_gpu_memory()
        end_system_memory = self.get_system_memory()
        
        # è®¡ç®—å„éƒ¨åˆ†æ—¶é—´
        # ä½¿ç”¨ç›‘æ§å‡½æ•°è¿”å›çš„å‡†ç¡®æ¨ç†æ—¶é—´
        if progress_data and progress_data.get('inference_start_time') and progress_data.get('inference_end_time'):
            actual_inference_time = progress_data['inference_end_time'] - progress_data['inference_start_time']
            print(f"ä½¿ç”¨ç›‘æ§å‡½æ•°æµ‹é‡çš„æ¨ç†æ—¶é—´: {actual_inference_time:.2f}ç§’")
        else:
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            actual_inference_time = completion_time - request_time
            print(f"ä½¿ç”¨ä¼ ç»Ÿæ–¹æ³•æµ‹é‡çš„æ¨ç†æ—¶é—´: {actual_inference_time:.2f}ç§’")
        
        total_inference_time = actual_inference_time  # ä½¿ç”¨å®é™…æ¨ç†æ—¶é—´
        request_time_taken = 0.0  # ComfyUIè¯·æ±‚æ—¶é—´å¾ˆçŸ­ï¼Œå¯ä»¥å¿½ç•¥
        processing_time = end_time - completion_time  # å›¾ç‰‡è·å–ç­‰åå¤„ç†æ—¶é—´
        
        # ç®€åŒ–ï¼šåªè®°å½•æ€»æ¨ç†æ—¶é—´ï¼Œä¸è®¡ç®—å„å±‚æ—¶é—´
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        gpu_memory_used = end_gpu_memory  # ä½¿ç”¨å®é™…ä½¿ç”¨çš„å†…å­˜ï¼Œè€Œä¸æ˜¯å˜åŒ–é‡
        system_memory_used = end_system_memory - start_system_memory
        
        # è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯:")
        print(f"  - start_gpu_memory: {start_gpu_memory:.2f}GB")
        print(f"  - end_gpu_memory: {end_gpu_memory:.2f}GB")
        print(f"  - gpu_memory_used: {gpu_memory_used:.2f}GB")
        print(f"  - gpu_id: {self.gpu_id}")
        
        print(f"ç»“æŸçŠ¶æ€ - GPUå†…å­˜: {end_gpu_memory:.2f}GB, ç³»ç»Ÿå†…å­˜: {end_system_memory:.2f}GB")
        if end_detailed_gpu:
            print(f"GPUåˆ©ç”¨ç‡: {end_detailed_gpu.get('utilization_percent', 0):.1f}%")
        
        result = {
            'prompt': prompt,
            'negative_prompt': negative_prompt,
            'steps': steps,
            'cfg': cfg,
            'total_inference_time': total_inference_time,
            'request_time': request_time_taken,
            'processing_time': processing_time,
            'start_gpu_memory': start_gpu_memory,
            'end_gpu_memory': end_gpu_memory,
            'gpu_memory_used': gpu_memory_used,
            'start_system_memory': start_system_memory,
            'end_system_memory': end_system_memory,
            'system_memory_used': system_memory_used,
            'model_parameters': model_info,
            'gpu_details_start': start_detailed_gpu,
            'gpu_details_end': end_detailed_gpu,
            'progress_data': progress_data,
            'gpu_id': self.gpu_id,
            'timestamp': datetime.now().isoformat()
        }
        
        print(f"æ¨ç†å®Œæˆ - æ€»æ—¶é—´: {total_inference_time:.2f}ç§’")
        print(f"  - è¯·æ±‚æ—¶é—´: {request_time_taken:.2f}ç§’")
        print(f"  - å¤„ç†æ—¶é—´: {processing_time:.2f}ç§’")
        print(f"GPUå†…å­˜ä½¿ç”¨: {gpu_memory_used:.2f}GB")
        print(f"ç³»ç»Ÿå†…å­˜å˜åŒ–: {system_memory_used:+.2f}GB")
        
        # æ‰“å°æ¨¡å‹å‚æ•°é‡ä¿¡æ¯
        if model_info:
            print(f"æ¨¡å‹å‚æ•°é‡:")
            if 'unet_parameters' in model_info:
                print(f"  - UNet: {model_info['unet_parameters']:,} å‚æ•° ({model_info.get('unet_size_gb', 0):.2f}GB)")
            if 'text_encoder_parameters' in model_info:
                print(f"  - Text Encoder: {model_info['text_encoder_parameters']:,} å‚æ•° ({model_info.get('text_encoder_size_gb', 0):.2f}GB)")
            if 'vae_parameters' in model_info:
                print(f"  - VAE: {model_info['vae_parameters']:,} å‚æ•° ({model_info.get('vae_size_gb', 0):.2f}GB)")
            if 'total_parameters' in model_info:
                print(f"  - æ€»è®¡: {model_info['total_parameters']:,} å‚æ•°")
        
        return result
    
    def run_batch_tests(self):
        """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
        test_configs = [
            {
                "prompt": "A beautiful anime character in a magical garden, detailed, high quality",
                "negative_prompt": "",
                "steps": 30,
                "cfg": 4.0
            },
            {
                "prompt": "A futuristic city with flying cars, cyberpunk style, anime",
                "negative_prompt": "blurry, low quality",
                "steps": 30,
                "cfg": 5.0
            },
            {
                "prompt": "A cute cat in a cozy room, warm lighting, detailed",
                "negative_prompt": "",
                "steps": 30,
                "cfg": 5.5
            }
        ]
        
        print("ç®€åŒ–ComfyUI Neta Luminaæ‰¹é‡æ¨ç†æµ‹è¯•")
        print("=" * 50)
        
        # æ£€æŸ¥ComfyUIçŠ¶æ€
        if not self.check_comfyui_status():
            print("âŒ ComfyUIæœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨ComfyUI")
            return []
        
        print("âœ… ComfyUIè¿æ¥æ­£å¸¸")
        
        # è¿è¡Œæµ‹è¯•
        for i, config in enumerate(test_configs, 1):
            print(f"\næµ‹è¯• {i}/{len(test_configs)}")
            result = self.run_inference_test(**config)
            if result:
                self.results.append(result)
                print(f"âœ… æµ‹è¯• {i} å®Œæˆ")
            else:
                print(f"âŒ æµ‹è¯• {i} å¤±è´¥")
        
        return self.results
    
    def save_results(self, filename=None):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        if not self.results:
            print("æ²¡æœ‰æµ‹è¯•ç»“æœå¯ä¿å­˜")
            return None
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"simple_comfyui_neta_lumina_results_{timestamp}.json"
        
        # ç›´æ¥ä¿å­˜ç»“æœï¼ˆä¸åŒ…å«å›¾ç‰‡æ•°æ®ï¼‰
        results_for_json = self.results
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_for_json, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… æµ‹è¯•ç»“æœå·²ä¿å­˜: {filename}")
        return filename
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        if not self.results:
            print("æ²¡æœ‰æµ‹è¯•ç»“æœ")
            return
        
        print("\n" + "=" * 50)
        print("ç®€åŒ–ComfyUI Neta Luminaæµ‹è¯•æ€»ç»“")
        print("=" * 50)
        
        total_tests = len(self.results)
        successful_tests = len([r for r in self.results if r])
        
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"æˆåŠŸæµ‹è¯•: {successful_tests}")
        print(f"å¤±è´¥æµ‹è¯•: {total_tests - successful_tests}")
        
        if successful_tests > 0:
            # åŸºæœ¬æ€§èƒ½ç»Ÿè®¡
            avg_total_time = sum(r.get('total_inference_time', 0) for r in self.results if r) / successful_tests
            avg_request_time = sum(r.get('request_time', 0) for r in self.results if r) / successful_tests
            avg_processing_time = sum(r.get('processing_time', 0) for r in self.results if r) / successful_tests
            
            print(f"\næ—¶é—´ç»Ÿè®¡:")
            print(f"  å¹³å‡æ€»æ¨ç†æ—¶é—´: {avg_total_time:.2f}ç§’")
            print(f"  å¹³å‡è¯·æ±‚æ—¶é—´: {avg_request_time:.2f}ç§’")
            print(f"  å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.2f}ç§’")
            
            # å†…å­˜ç»Ÿè®¡
            avg_gpu_memory = sum(r.get('gpu_memory_used', 0) for r in self.results if r) / successful_tests
            avg_system_memory = sum(r.get('system_memory_used', 0) for r in self.results if r) / successful_tests
            
            print(f"\nå†…å­˜ç»Ÿè®¡:")
            print(f"  å¹³å‡GPUå†…å­˜ä½¿ç”¨: {avg_gpu_memory:.2f}GB")
            print(f"  å¹³å‡ç³»ç»Ÿå†…å­˜ä½¿ç”¨: {avg_system_memory:.2f}GB")
            
            # æ¨¡å‹å‚æ•°é‡ä¿¡æ¯ï¼ˆä»ç¬¬ä¸€ä¸ªç»“æœè·å–ï¼‰
            first_result = self.results[0]
            if 'model_parameters' in first_result and first_result['model_parameters']:
                model_info = first_result['model_parameters']
                print(f"\næ¨¡å‹å‚æ•°é‡:")
                if 'unet_parameters' in model_info:
                    print(f"  UNet: {model_info['unet_parameters']:,} å‚æ•° ({model_info.get('unet_size_gb', 0):.2f}GB)")
                if 'text_encoder_parameters' in model_info:
                    print(f"  Text Encoder: {model_info['text_encoder_parameters']:,} å‚æ•° ({model_info.get('text_encoder_size_gb', 0):.2f}GB)")
                if 'vae_parameters' in model_info:
                    print(f"  VAE: {model_info['vae_parameters']:,} å‚æ•° ({model_info.get('vae_size_gb', 0):.2f}GB)")
                if 'total_parameters' in model_info:
                    print(f"  æ€»è®¡: {model_info['total_parameters']:,} å‚æ•°")
            
            # GPUåˆ©ç”¨ç‡ç»Ÿè®¡
            gpu_utilizations = []
            for r in self.results:
                if r and 'gpu_details_start' in r and r['gpu_details_start']:
                    gpu_utilizations.append(r['gpu_details_start'].get('utilization_percent', 0))
            
            if gpu_utilizations:
                avg_gpu_util = sum(gpu_utilizations) / len(gpu_utilizations)
                print(f"\nGPUåˆ©ç”¨ç‡: {avg_gpu_util:.1f}%")
        
        print("\nè¯¦ç»†ç»“æœ:")
        for i, result in enumerate(self.results, 1):
            if result:
                print(f"æµ‹è¯• {i}:")
                print(f"  æ€»æ¨ç†æ—¶é—´: {result.get('total_inference_time', 0):.2f}ç§’")
                print(f"  è¯·æ±‚æ—¶é—´: {result.get('request_time', 0):.2f}ç§’")
                print(f"  å¤„ç†æ—¶é—´: {result.get('processing_time', 0):.2f}ç§’")
                print(f"  GPUå†…å­˜ä½¿ç”¨: {result.get('gpu_memory_used', 0):.2f}GB")
                print(f"  ç³»ç»Ÿå†…å­˜ä½¿ç”¨: {result.get('system_memory_used', 0):+.2f}GB")
                print(f"  æç¤ºè¯: {result.get('prompt', 'N/A')[:50]}...")
            else:
                print(f"æµ‹è¯• {i}: å¤±è´¥")
        
        print("=" * 50)

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ç®€åŒ–ComfyUI Neta Luminaæµ‹è¯•å·¥å…·")
    parser.add_argument("--port", type=int, default=8188, help="ComfyUIç«¯å£")
    parser.add_argument("--gpu-id", type=int, default=0, help="æŒ‡å®šä½¿ç”¨çš„GPU ID")
    parser.add_argument("--prompt", default="A beautiful anime character in a magical garden, detailed, high quality", 
                       help="æ¨ç†æç¤ºè¯")
    parser.add_argument("--negative-prompt", default="", help="è´Ÿé¢æç¤ºè¯")
    parser.add_argument("--steps", type=int, default=30, help="æ¨ç†æ­¥æ•°")
    parser.add_argument("--cfg", type=float, default=4.0, help="CFGå€¼")
    parser.add_argument("--batch", action="store_true", help="è¿è¡Œæ‰¹é‡æµ‹è¯•")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = SimpleComfyUITester(args.port, args.gpu_id)
    
    if args.batch:
        # æ‰¹é‡æµ‹è¯•
        results = tester.run_batch_tests()
        tester.print_summary()
        if results:
            tester.save_results()
    else:
        # å•æ¬¡æµ‹è¯•
        result = tester.run_inference_test(
            prompt=args.prompt,
            negative_prompt=args.negative_prompt,
            steps=args.steps,
            cfg=args.cfg
        )
        
        if result:
            tester.results = [result]
            tester.print_summary()
            tester.save_results()
        else:
            print("âŒ æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main()
