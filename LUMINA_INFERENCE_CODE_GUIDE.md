# LUMINAæ¨¡å‹æ¨ç†ä»£ç è¯¦è§£

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜LUMINA-Image-2.0æ¨¡å‹çš„æ¨ç†ä»£ç å®ç°ï¼ŒåŒ…æ‹¬æ¨¡å‹åŠ è½½ã€å‚æ•°é…ç½®ã€æ¨ç†æ‰§è¡Œå’Œæ€§èƒ½æµ‹é‡ç­‰å…³é”®éƒ¨åˆ†ã€‚

## 1. æ¨¡å‹åŠ è½½ä»£ç 

### 1.1 åŸºç¡€æ¨¡å‹åŠ è½½

```python
# æ–‡ä»¶: inference_benchmark.py (ç¬¬775-782è¡Œ)
from diffusers import Lumina2Pipeline

print("æ­£åœ¨åŠ è½½Luminaæ¨¡å‹...")
pipe = Lumina2Pipeline.from_pretrained(
    "./Lumina-Image-2.0",           # æ¨¡å‹è·¯å¾„
    torch_dtype=torch.bfloat16      # ä½¿ç”¨bfloat16ç²¾åº¦
)
pipe.enable_model_cpu_offload()     # å¯ç”¨CPUå¸è½½ä»¥èŠ‚çœGPUå†…å­˜
```

**å…³é”®å‚æ•°è¯´æ˜**ï¼š
- `torch_dtype=torch.bfloat16`: ä½¿ç”¨bfloat16ç²¾åº¦ï¼Œå¹³è¡¡æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨
- `enable_model_cpu_offload()`: å¯ç”¨CPUå¸è½½ï¼Œå°†æœªä½¿ç”¨çš„æ¨¡å‹ç»„ä»¶ç§»åˆ°CPU

### 1.2 æ¨¡å‹é…ç½®æ£€æŸ¥

```python
# æ£€æŸ¥æ¨¡å‹å±æ€§
print(f"æ¨¡å‹ç±»å‹: {type(pipe)}")
print(f"æ¨¡å‹å±æ€§: {dir(pipe)}")

# æ£€æŸ¥å…³é”®ç»„ä»¶
print(f"æ£€æŸ¥text_encoderå±æ€§: {hasattr(pipe, 'text_encoder')}")
print(f"æ£€æŸ¥transformerå±æ€§: {hasattr(pipe, 'transformer')}")
print(f"æ£€æŸ¥vaeå±æ€§: {hasattr(pipe, 'vae')}")
```

## 2. æ¨ç†å‚æ•°é…ç½®

### 2.1 å®é™…æµ‹é‡æ¨¡å¼å‚æ•°

```python
# æ–‡ä»¶: inference_benchmark.py (ç¬¬357-367è¡Œ)
elif model_name == "Lumina":
    kwargs = {
        'prompt': prompt,                    # è¾“å…¥æç¤ºè¯
        'height': size[0],                   # å›¾åƒé«˜åº¦
        'width': size[1],                    # å›¾åƒå®½åº¦
        'num_inference_steps': steps,        # æ¨ç†æ­¥æ•°
        'guidance_scale': 4.0,              # CFGå¼•å¯¼å¼ºåº¦
        'cfg_trunc_ratio': 1.0,             # CFGæˆªæ–­æ¯”ä¾‹
        'cfg_normalization': True,          # CFGé‡æ–°å½’ä¸€åŒ–
        'max_sequence_length': 256          # æœ€å¤§åºåˆ—é•¿åº¦
    }
```

### 2.2 åŸºå‡†æµ‹è¯•æ¨¡å¼å‚æ•°

```python
# æ–‡ä»¶: inference_benchmark.py (ç¬¬604-614è¡Œ)
elif model_name == "Lumina":
    kwargs = {
        'prompt': prompt,                    # è¾“å…¥æç¤ºè¯
        'height': size[0],                   # å›¾åƒé«˜åº¦
        'width': size[1],                    # å›¾åƒå®½åº¦
        'num_inference_steps': steps,        # æ¨ç†æ­¥æ•°
        'guidance_scale': 4.0,              # CFGå¼•å¯¼å¼ºåº¦
        'cfg_trunc_ratio': 1.0,             # CFGæˆªæ–­æ¯”ä¾‹
        'cfg_normalization': True,          # CFGé‡æ–°å½’ä¸€åŒ–
        'max_sequence_length': 256          # æœ€å¤§åºåˆ—é•¿åº¦
    }
```

## 3. æ¨ç†æ‰§è¡Œä»£ç 

### 3.1 åŸºç¡€æ¨ç†æ‰§è¡Œ

```python
# æ–‡ä»¶: inference_benchmark.py (ç¬¬616-619è¡Œ)
# å®é™…æµ‹é‡æ¨ç†æ—¶é—´
start_time = time.time()
image = pipe(**kwargs).images[0]    # æ‰§è¡Œæ¨ç†å¹¶è·å–ç¬¬ä¸€å¼ å›¾åƒ
total_time = time.time() - start_time

print(f"å®é™…æµ‹é‡æ€»æ¨ç†æ—¶é—´: {total_time:.2f}ç§’")
```

### 3.2 å¸¦Hookæµ‹é‡çš„æ¨ç†æ‰§è¡Œ

```python
# æ–‡ä»¶: inference_benchmark.py (ç¬¬369-373è¡Œ)
# æ‰§è¡Œæ¨ç†å¹¶æµ‹é‡æ—¶é—´
print("æ‰§è¡Œæ¨ç†å¹¶å®é™…æµ‹é‡å„å±‚æ—¶é—´...")
total_start = time.time()
image = pipe(**kwargs).images[0]    # æ‰§è¡Œæ¨ç†
total_end = time.time()

# è®¡ç®—æ€»æ¨ç†æ—¶é—´
total_time = total_end - total_start
```

## 4. æ€§èƒ½æµ‹é‡ä»£ç 

### 4.1 Hookæ³¨å†Œä»£ç 

```python
# æ–‡ä»¶: inference_benchmark.py (ç¬¬246-301è¡Œ)
# æ³¨å†ŒUNet/Transformer Hook
print(f"æ£€æŸ¥unetå±æ€§: {hasattr(pipe, 'unet')}")
print(f"æ£€æŸ¥transformerå±æ€§: {hasattr(pipe, 'transformer')}")

# FLUXä½¿ç”¨transformerï¼Œå…¶ä»–æ¨¡å‹ä½¿ç”¨unet
unet_module = None
if hasattr(pipe, 'unet'):
    unet_module = pipe.unet
    print("æ³¨å†ŒUNet Hook...")
elif hasattr(pipe, 'transformer'):
    unet_module = pipe.transformer
    print("æ³¨å†ŒTransformer Hook...")

if unet_module is not None:
    unet_modules = list(unet_module.named_modules())
    print(f"UNet/Transformeræ¨¡å—æ•°é‡: {len(unet_modules)}")
    
    # ä¸ºä¸»è¦ç»„ä»¶æ³¨å†ŒHook
    if hasattr(unet_module, 'noise_refiner'):
        hook = unet_module.noise_refiner.register_forward_hook(unet_hook)
        hooks.append(hook)
        print(f"  - æ³¨å†Œä¸»è¦UNetç»„ä»¶: noise_refiner")
```

### 4.2 VAE Hookæ³¨å†Œ

```python
# æ–‡ä»¶: inference_benchmark.py (ç¬¬290-336è¡Œ)
# æ³¨å†ŒVAE Hook
print(f"æ£€æŸ¥vaeå±æ€§: {hasattr(pipe, 'vae')}")
if hasattr(pipe, 'vae'):
    print("æ³¨å†ŒVAE Hook...")
    vae_modules = list(pipe.vae.named_modules())
    print(f"VAEæ¨¡å—æ•°é‡: {len(vae_modules)}")
    
    # ä¸ºä¸»è¦çš„VAEç»„ä»¶æ³¨å†ŒHook
    if hasattr(pipe.vae, 'decoder'):
        hook = pipe.vae.decoder.register_forward_hook(vae_hook)
        hooks.append(hook)
        print(f"  - æ³¨å†Œä¸»è¦VAEç»„ä»¶: decoder")
    
    if hasattr(pipe.vae, 'up_blocks'):
        for i, block in enumerate(pipe.vae.up_blocks[:2]):
            hook = block.register_forward_hook(vae_hook)
            hooks.append(hook)
            print(f"  - æ³¨å†ŒVAE Up Block: {i}")
```

## 5. æ—¶é—´æµ‹é‡å’Œåˆ†æ

### 5.1 Hookæ—¶é—´æµ‹é‡

```python
# æ–‡ä»¶: inference_benchmark.py (ç¬¬199-227è¡Œ)
def unet_hook(module, input, output):
    nonlocal unet_start, unet_end
    if unet_start == 0:
        unet_start = time.time()
    unet_end = time.time()
    print(f"  ğŸ” UNet Hookè°ƒç”¨: {module.__class__.__name__}")

def vae_hook(module, input, output):
    nonlocal vae_decode_start, vae_decode_end
    if vae_decode_start == 0:
        vae_decode_start = time.time()
    vae_decode_end = time.time()
    print(f"  ğŸ” VAE Hookè°ƒç”¨: {module.__class__.__name__}")

def attention_hook(module, input, output):
    start_time = time.time()
    # ç­‰å¾…GPUè®¡ç®—å®Œæˆ
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    end_time = time.time()
    attention_times.append(end_time - start_time)
```

### 5.2 æ—¶é—´è®¡ç®—å’ŒéªŒè¯

```python
# æ–‡ä»¶: inference_benchmark.py (ç¬¬383-392è¡Œ)
# éªŒè¯æ—¶é—´è®¡ç®—ä¸€è‡´æ€§
calculated_total = layer_times['text_encoding_time'] + layer_times['unet_time'] + layer_times['vae_decode_time']
time_diff = abs(total_time - calculated_total)
if time_diff > 0.1:  # å¦‚æœå·®å¼‚è¶…è¿‡0.1ç§’
    print(f"  âš ï¸ æ—¶é—´è®¡ç®—ä¸ä¸€è‡´: æ€»æ—¶é—´{total_time:.3f}ç§’ vs è®¡ç®—æ—¶é—´{calculated_total:.3f}ç§’ (å·®å¼‚{time_diff:.3f}ç§’)")
    # ä½¿ç”¨å®é™…æµ‹é‡çš„æ€»æ—¶é—´
    layer_times['total_inference_time'] = total_time
else:
    layer_times['total_inference_time'] = calculated_total
    print(f"  âœ… æ—¶é—´è®¡ç®—ä¸€è‡´: {calculated_total:.3f}ç§’")
```

## 6. å®Œæ•´çš„æ¨ç†æµç¨‹

### 6.1 å•æ¬¡æ¨ç†å®Œæ•´æµç¨‹

```python
def _benchmark_single_inference(self, pipe, prompt: str, size: Tuple[int, int], steps: int, model_name: str) -> Dict:
    """å•æ¬¡æ¨ç†åŸºå‡†æµ‹è¯•"""
    print(f"å¼€å§‹{model_name}æ¨ç†ï¼ˆå®é™…æµ‹é‡æ¨¡å¼ï¼‰...")
    
    try:
        # 1. ä½¿ç”¨å®é™…æµ‹é‡æ–¹æ³•
        layer_times = self._measure_actual_layer_times(pipe, prompt, size, steps, model_name)
        
        if layer_times is None:
            raise Exception("å®é™…æµ‹é‡å¤±è´¥")
        
        # 2. ä½¿ç”¨å®é™…æµ‹é‡çš„æ€»æ¨ç†æ—¶é—´
        total_inference_time = layer_times.get('total_inference_time', sum([
            layer_times.get('text_encoding_time', 0),
            layer_times.get('unet_time', 0),
            layer_times.get('vae_decode_time', 0)
        ]))
        
        # 3. æ‰“å°æ¨ç†ç»“æœ
        print(f"æ¨ç†å®Œæˆï¼Œæ€»è€—æ—¶: {total_inference_time:.2f}ç§’")
        print(f"  - Text Encoding: {layer_times.get('text_encoding_time', 0):.2f}ç§’")
        print(f"  - UNetæ¨ç†: {layer_times.get('unet_time', 0):.2f}ç§’")
        print(f"    - Attentionå±‚: {layer_times.get('attention_time', 0):.2f}ç§’")
        print(f"    - å…¶ä»–å±‚: {layer_times.get('other_layers_time', 0):.2f}ç§’")
        print(f"  - VAEè§£ç : {layer_times.get('vae_decode_time', 0):.2f}ç§’")
        
        # 4. ä¿å­˜ç”Ÿæˆçš„å›¾ç‰‡
        save_start_time = time.time()
        safe_prompt = "".join(c for c in prompt[:30] if c.isalnum() or c in (' ', '-', '_')).rstrip()
        filename = f"{model_name.lower().replace(' ', '_')}_{size[0]}x{size[1]}_steps_{steps}_cfg_{4.0}_{safe_prompt}.png"
        image_path = self.output_dir / filename
        layer_times['image'].save(image_path)
        save_time = time.time() - save_start_time
        print(f"ä¿å­˜å›¾ç‰‡: {image_path} (è€—æ—¶: {save_time:.2f}ç§’)")
        
        # 5. è¿”å›ç»“æœ
        return {
            'prompt': prompt,
            'size': size,
            'steps': steps,
            'inference_time': total_inference_time,
            'total_time': time.time() - start_time,
            'save_time': save_time,
            'layer_times': layer_times,
            'success': True
        }
        
    except Exception as e:
        print(f"æ¨ç†å¤±è´¥: {e}")
        return {
            'prompt': prompt,
            'size': size,
            'steps': steps,
            'inference_time': 0,
            'total_time': 0,
            'save_time': 0,
            'layer_times': {},
            'success': False,
            'error': str(e)
        }
```

## 7. åŸºå‡†æµ‹è¯•ä¸»å¾ªç¯

### 7.1 å¤šè½®æµ‹è¯•å¾ªç¯

```python
def _real_lumina_benchmark(self):
    """çœŸå®LuminaåŸºå‡†æµ‹è¯•"""
    print("å¼€å§‹çœŸå®Luminaæ¨¡å‹æµ‹è¯•...")
    
    try:
        # 1. åŠ è½½æ¨¡å‹
        from diffusers import Lumina2Pipeline
        pipe = Lumina2Pipeline.from_pretrained(
            "./Lumina-Image-2.0",
            torch_dtype=torch.bfloat16
        )
        pipe.enable_model_cpu_offload()
        
        # 2. æ‰§è¡Œå¤šè½®æµ‹è¯•
        results = []
        for prompt in self.test_prompts:
            for size in self.test_sizes:
                for steps in self.model_recommended_steps["Lumina"]:
                    result = self._benchmark_single_inference(
                        pipe, prompt, size, steps, "Lumina"
                    )
                    results.append(result)
        
        # 3. è®¡ç®—å¹³å‡æ€§èƒ½
        return {
            'model': 'Lumina (Real Test)',
            'results': results,
            'avg_time': np.mean([r['inference_time'] for r in results]),
            'avg_layer_times': self._calculate_avg_layer_times(results)
        }
        
    except Exception as e:
        print(f"Luminaæ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return {
            'model': 'Lumina (Failed)',
            'results': [],
            'avg_time': 0,
            'avg_layer_times': {}
        }
```

## 8. å…³é”®é…ç½®å‚æ•°

### 8.1 æ¨¡å‹æ¨èå‚æ•°

```python
# æ–‡ä»¶: inference_benchmark.py (ç¬¬50-60è¡Œ)
self.model_recommended_steps = {
    "FLUX": [50],
    "Lumina": [30],           # LUMINAæ¨è30æ­¥
    "Neta Lumina": [20]       # Neta Luminaæ¨è20æ­¥
}

self.test_sizes = [
    (1024, 1024),            # æ ‡å‡†å°ºå¯¸
    (1024, 1024),            # é‡å¤æµ‹è¯•
    (1024, 1024)             # ä¸‰æ¬¡æµ‹è¯•
]

self.test_prompts = [
    "A beautiful landscape with mountains and lakes, photorealistic, high quality",
    "A futuristic city with flying cars, cyberpunk style, anime",
    "A cute anime character in a magical garden, detailed, high quality"
]
```

### 8.2 CFGä¼˜åŒ–å‚æ•°è¯¦è§£

```python
# LUMINAä¸“ç”¨CFGå‚æ•°
'guidance_scale': 4.0,        # CFGå¼•å¯¼å¼ºåº¦ï¼Œå¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§
'cfg_trunc_ratio': 1.0,       # CFGæˆªæ–­æ¯”ä¾‹ï¼Œ1.0è¡¨ç¤ºå®Œå…¨å¯ç”¨
'cfg_normalization': True,    # CFGé‡æ–°å½’ä¸€åŒ–ï¼Œæé«˜ç¨³å®šæ€§
'max_sequence_length': 256    # æœ€å¤§æ–‡æœ¬åºåˆ—é•¿åº¦
```

## 9. é”™è¯¯å¤„ç†å’Œè°ƒè¯•

### 9.1 å¼‚å¸¸å¤„ç†

```python
try:
    # æ‰§è¡Œæ¨ç†
    image = pipe(**kwargs).images[0]
except Exception as e:
    print(f"æ¨ç†å¤±è´¥: {e}")
    return {
        'success': False,
        'error': str(e)
    }
```

### 9.2 è°ƒè¯•ä¿¡æ¯è¾“å‡º

```python
print(f"Hookæµ‹é‡ç»“æœ:")
print(f"  - Text Encoding: {text_encoding_start:.3f} -> {text_encoding_end:.3f}")
print(f"  - UNet: {unet_start:.3f} -> {unet_end:.3f}")
print(f"  - VAE: {vae_decode_start:.3f} -> {vae_decode_end:.3f}")
print(f"  - Attentionè°ƒç”¨æ¬¡æ•°: {len(attention_times)}")
print(f"  - å…¶ä»–å±‚è°ƒç”¨æ¬¡æ•°: {len(other_layer_times)}")
print(f"  - æ€»æ¨ç†æ—¶é—´: {total_time:.3f}ç§’")
```

## 10. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 10.1 å†…å­˜ä¼˜åŒ–

```python
# å¯ç”¨CPUå¸è½½
pipe.enable_model_cpu_offload()

# ä½¿ç”¨bfloat16ç²¾åº¦
torch_dtype=torch.bfloat16
```

### 10.2 æ¨ç†ä¼˜åŒ–

```python
# è°ƒæ•´CFGå‚æ•°
'guidance_scale': 3.5,        # é™ä½å¼•å¯¼å¼ºåº¦ï¼Œæé«˜é€Ÿåº¦
'cfg_trunc_ratio': 1.0,       # å¯ç”¨æˆªæ–­ä¼˜åŒ–
'num_inference_steps': 20     # å‡å°‘æ¨ç†æ­¥æ•°
```

---

**æ³¨æ„**: ä»¥ä¸Šä»£ç ç‰‡æ®µå‡æ¥è‡ª `inference_benchmark.py` æ–‡ä»¶ï¼Œå±•ç¤ºäº†LUMINAæ¨¡å‹æ¨ç†çš„å®Œæ•´å®ç°è¿‡ç¨‹ã€‚
